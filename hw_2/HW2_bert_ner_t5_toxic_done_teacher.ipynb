{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b06ba3f-3c98-4fd7-948d-362d3260fc5d",
   "metadata": {},
   "source": [
    "# –í–≤–µ–¥–µ–Ω–∏–µ –≤ NLP, —á–∞—Å—Ç—å 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f3154e",
   "metadata": {},
   "source": [
    "\n",
    "## NER c BERT (25 –±–∞–ª–ª–æ–≤)\n",
    "\n",
    "1. –í–∑—è—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –î–ó –∏ –æ–±—É—á–∏—Ç—å –Ω–∞ –Ω—ë–º BERT.\n",
    "2. –û–±—É—á–∏—Ç—å BERT –Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "3. –û—Ü–µ–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç, —Å—Ä–∞–≤–Ω–∏—Ç—å —Å –º–æ–¥–µ–ª—å—é –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –î–ó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a696ec-f1af-4779-bb81-a4aa644fddfd",
   "metadata": {},
   "source": [
    "### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (5 –±–∞–ª–ª–æ–≤)\n",
    "\n",
    "–ü–æ–¥—É–º–∞—Ç—å –æ:\n",
    "1. –ö–∞–∫ subword —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–æ–≤–ª–∏—è–µ—Ç –Ω–∞ BIO —Ä–∞–∑–∑–º–µ—Ç–∫—É?\n",
    "2. –ß—Ç–æ –¥–µ–ª–∞—Ç—å —Å `[CLS]` –∏ `[SEP]` —Ç–æ–∫–µ–Ω–∞–º–∏? (–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç `DataCollatorForTokenClassification`)\n",
    "\n",
    "> Hint! –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä —É–º–µ–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –ø—Ä–µ–¥—Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã–º–∏ –Ω–∞ ¬´—Å–ª–æ–≤–∞¬ª —Ç–µ–∫—Å—Ç–∞–º–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "066c1544-367c-492b-b7be-4e886babfc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "BASE_NER_MODEL = \"bert-base-cased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(BASE_NER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d13c09d0-a7ab-4fe3-aac4-fb8ba4b5f1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003 = load_dataset(\"conll2003\")\n",
    "conll2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a1ac5b-0722-4387-a885-80b927fbc10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '100',\n",
       " 'tokens': ['Rabinovich',\n",
       "  'is',\n",
       "  'winding',\n",
       "  'up',\n",
       "  'his',\n",
       "  'term',\n",
       "  'as',\n",
       "  'ambassador',\n",
       "  '.'],\n",
       " 'pos_tags': [21, 42, 39, 33, 29, 21, 15, 21, 7],\n",
       " 'chunk_tags': [11, 21, 22, 15, 11, 12, 13, 11, 0],\n",
       " 'ner_tags': [1, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = conll2003[\"train\"][100]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573be60b-3c59-4a4c-9cc7-2cab8d9f2a2d",
   "metadata": {},
   "source": [
    "* tokens - –∏—Å—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –±—ã–ª–∞ —Å–¥–µ–ª–∞–Ω–∞ NER-—Ä–∞–∑–º–µ—Ç–∫–∞\n",
    "* ner_tags - –≤–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ NER-—Ç—ç–≥–æ–≤\n",
    "* pos_tags - —Ä–∞–∑–º–µ—Ç–∫–∞ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏, –∫–æ—Ç–æ—Ä—É—é –º—ã –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º\n",
    "* chunk_tags - —Ä–∞–∑–º–µ—Ç–∫–∞ —á–∞–Ω–∫–æ–≤, –∫–æ—Ç–æ—Ä—É—é –º—ã –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c070226-e70f-4df1-b770-f403a19ef205",
   "metadata": {},
   "source": [
    "–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –º–æ–∂–µ—Ç –ø—Ä–µ–≤—ã—à–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ö–æ–¥–Ω—ã—Ö –ª–µ–π–±–ª–æ–≤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f4628f-7e0f-4113-ac0a-04f81f77271c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Ra',\n",
       " '##bino',\n",
       " '##vich',\n",
       " 'is',\n",
       " 'winding',\n",
       " 'up',\n",
       " 'his',\n",
       " 'term',\n",
       " 'as',\n",
       " 'ambassador',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer(example[\"tokens\"], is_split_into_words=True).tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb99e70-85e2-438f-8c68-e15a6701ccfa",
   "metadata": {},
   "source": [
    "–ó–Ω–∞—á–µ–Ω–∏–µ —Ç—ç–≥–∞ –≤ `ner_tags` –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –≤ –º–µ—Ç–∫—É NER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b8fce68-13e8-4093-a147-15ae865ac73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER TAGS [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None)\n"
     ]
    }
   ],
   "source": [
    "print(\"NER TAGS\", example[\"ner_tags\"])\n",
    "print(conll2003[\"train\"].features[\"ner_tags\"].feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27a3addb-073d-4a2f-bd49-caa3411a4369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã\n",
      "['Rabinovich', 'is', 'winding', 'up', 'his', 'term', 'as', 'ambassador', '.']\n",
      "–í–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ NER –º–µ—Ç–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "–¢–µ–∫—Å—Ç–æ–≤—ã–µ NER –º–µ—Ç–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤\n",
      "['B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "–¢–æ–∫–µ–Ω—ã –ø–æ—Å–ª–µ —Ä–∞–±–æ—Ç—ã —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ BERT\n",
      "['[CLS]', 'Ra', '##bino', '##vich', 'is', 'winding', 'up', 'his', 'term', 'as', 'ambassador', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(\"–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã\")\n",
    "print(example[\"tokens\"])\n",
    "print(\"–í–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ NER –º–µ—Ç–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "print(example[\"ner_tags\"])\n",
    "tags_str = []\n",
    "features = conll2003[\"train\"].features[\"ner_tags\"].feature\n",
    "for tag in example[\"ner_tags\"]:\n",
    "    tags_str.append(features.int2str(tag))\n",
    "print(\"–¢–µ–∫—Å—Ç–æ–≤—ã–µ NER –º–µ—Ç–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "print(tags_str)\n",
    "print(\"–¢–æ–∫–µ–Ω—ã –ø–æ—Å–ª–µ —Ä–∞–±–æ—Ç—ã —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ BERT\")\n",
    "print(bert_tokenizer(example[\"tokens\"], is_split_into_words=True).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48468ae1-48d3-4641-ada4-0a8cf25e50ad",
   "metadata": {},
   "source": [
    "–í—Å–ø–æ–º–Ω–∏–º –Ω–µ–º–Ω–æ–≥–æ, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç –º–µ—Ç–∫–∏ –≤ –∑–∞–¥–∞—á–µ –º–µ—Ä –≤ –∫–æ–¥–∏—Ä–æ–≤–∫–µ IOB. –í –¥–∞–Ω–Ω–æ–π –∑–∞–¥–∞—á–µ —É –Ω–∞—Å –µ—Å—Ç—å 4 —Ç–∏–ø–∞ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π:\n",
    "* PER - –ø–µ—Ä—Å–æ–Ω–∞\n",
    "* ORG - –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è\n",
    "* LOC - –ª–æ–∫–∞—Ü–∏—è\n",
    "* MISC - –¥—Ä—É–≥–æ–µ\n",
    "* O - –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç–∏\n",
    "\n",
    "–£ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö 2 –ø—Ä–µ—Ñ–∏–∫—Å–∞:\n",
    "* `B-` - beginning, —Ç.–µ. –Ω–∞—á–∞–ª–æ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç–∏.\n",
    "* `I-` - inside, —Ç.–µ. –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ä–∞–Ω–µ–µ –Ω–∞—á–∞—Ç–æ–π –∏–º–µ–Ω–æ–≤–∞–Ω–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç—å—é.\n",
    "\n",
    "–í –∏—Å—Ö–æ–¥–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
    "\n",
    "`['Rabinovich', 'is', 'winding', 'up', 'his', 'term', 'as', 'ambassador', '.']`\n",
    "–º–µ—Ç–∫–∏ –≤—ã–≥–ª—è–¥—è—Ç –∫–∞–∫ \n",
    "\n",
    "`['B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']`\n",
    "—Ç.–µ. `Rabinovich` —è–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä—Å–æ–Ω–æ–π. –ù–∞ —Å–ª–µ–¥—É—é—â–µ–º —Ç–æ–∫–µ–Ω–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω–∞—è —Å—É—â–Ω–æ—Å—Ç—å –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è, —Ç.–∫. —É –Ω–µ–≥–æ –º–µ—Ç–∫–∞ `O`.\n",
    "\n",
    "–ü–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ BERT –Ω–∞—à —Å—ç–º–ø–ª –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ —Å–ª–µ–¥—É—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã:\n",
    "\n",
    "`['[CLS]', 'Ra', '##bino', '##vich', 'is', 'winding', 'up', 'his', 'term', 'as', 'ambassador', '.', '[SEP]']`\n",
    "–û–±—Ä–∞—Ç–∏–º –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –æ–¥–∏–Ω —Ç–æ–∫–µ–Ω `Rabinovich` —Å –º–µ—Ç–∫–æ–π `B-PER` –±—ã–ª —Ä–∞–∑–±–∏—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º –±–µ—Ä—Ç–∞ –Ω–∞ 3 —Ç–æ–∫–µ–Ω–∞: `'Ra', '##bino', '##vich'`. –ò–º –Ω—É–∂–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ 3 –º–µ—Ç–∫–∏: `B-PER, I-PER, I-PER`, —Ç.–µ. –º—ã —Ä–∞–∑–±–∏–≤–∞–µ–º –º–µ—Ç–∫—É –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –Ω–∞ –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã.\n",
    "\n",
    "–¢–∞–∫–∂–µ –æ–±—Ä–∞—Ç–∏–º –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –ø–µ—Ä–≤—ã–π –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω - —ç—Ç–æ —Å–ø–µ—Ü—Å—Ç–æ–∫–µ–Ω—ã BERT –æ–∑–Ω–∞—á–∞—é—â–∏–µ –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞. –ò–º –º–æ–∂–Ω–æ –¥–∞—Ç—å –º–µ—Ç–∫–∏ `O`, —Ç.–∫. –æ–Ω–∏ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —á–∞—Å—Ç—å—é –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, –Ω–æ –º—ã –±—É–¥–µ–º –¥–∞–≤–∞—Ç—å –∏–º –æ—Å–æ–±–æ–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ -100. –í [–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ pytroch](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) —É –∫—Ä–æ—Å—Å—ç–Ω—Ç—Ä–æ–ø–∏–π–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å —ç—Ç–æ –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ `ignore_index`, —Ç.–µ. –º–µ—Ç–∫–∏, –∫–æ—Ç–æ—Ä—É—é –º—ã –±—É–¥–µ–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å. –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ transformers —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –Ω–∞ —Ç–æ–∫–µ–Ω–∞—Ö, —É –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–æ–∏—Ç -100 –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ NER-—Ç—ç–≥–∞, –Ω–µ –±—É–¥–µ—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ, –æ–Ω–∏ –±—É–¥—É—Ç –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω—ã.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ac7b9-d65f-4e09-8165-de5442a6ab6e",
   "metadata": {},
   "source": [
    "–ù–∞–ø–∏—à–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é `preprocess_ner_dataset`, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç `ner_tags` –¥–ª—è —Å–ª–æ–≤ –≤ —Ç—ç–≥–∏ –¥–ª—è BERT-—Ç–æ–∫–µ–Ω–æ–≤ –∏ –≥–æ—Ç–æ–≤–∏—Ç –æ—Å—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –¥–≤–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–ª–∏ –Ω–∞–ø–∏—Å–∞—Ç—å –≤—Å—ë –≤ –æ–¥–Ω–æ–π). –í —Ä–µ–∑—É–ª—Ç–∞—Ç–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è `conll2003.map(preprocess_ner_dataset)`, –≤ –∫–∞–∂–¥–æ–º –ø—Ä–∏–º–µ—Ä–µ:\n",
    "1. –î–æ–±–∞–≤–ª—è–µ—Ç—Å—è —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—Ö–æ–¥ (`input_ids`, `token_type_ids` –∏ `attention_mask`). –ü—Ä–∏ –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–∏–∏ —ç—Ç–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤—Ä—É—á–Ω—É—é –Ω—É–∂–Ω–æ –ø—Ä–æ—Å—Ç–∞–≤–∏—Ç—å `attention_mask` –ø–æ–ª–Ω–æ—Å—Ç—å—é –µ–¥–∏–Ω–∏—Ü–∞–º–∏, —Ç.–∫. –≤ –ø–∞–¥–¥–∏–Ω–≥–∏ –≤ —Å—ç–º–ø–ª–∞—Ö –ø–æ—è–≤–ª—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ —Ä–∞–º–∫–∞—Ö –±–∞—Ç—á–µ–π, –∞ `token_type_ids` –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω—É–ª—è–º–∏.\n",
    "2. `ner_tags` —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç—Å—è –≤ `labels` –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "\n",
    "–ß—Ç–æ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:\n",
    "* —É –æ–±—ä–µ–∫—Ç–∞ `conll2003[\"train\"].features[\"ner_tags\"].feature` –µ—Å—Ç—å –º–µ—Ç–æ–¥—ã `int2str` –∏ `str2int` –¥–ª—è –ø—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ NER-—Ç—ç–≥–∞ –≤ —Å—Ç—Ä–æ–∫–æ–≤—ã–π –≤–∏–¥ –∏ –æ–±—Ä–∞—Ç–Ω–æ\n",
    "* –°–ø–µ—Ü—Ç–æ–∫–µ–Ω–∞–º BERT –Ω—É–∂–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ -100\n",
    "* –≤—ã–∑–æ–≤ `bert_tokenizer(bert_tokenizer(example[\"tokens\"], is_split_into_words=True)` –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–∞–º input_ids, attention_mask, token_type_ids\n",
    "* –í—ã–∑–æ–≤ `bert_tokenizer(example[\"tokens\"], is_split_into_words=True, return_offsets_mapping=True))` –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ offset_mapping, –ø–æ–∑–∏—Ü–∏–∏ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\n",
    "* `bert_tokenizer.vocab` - –¥–ª—è –ø—Ä–µ–≤—Ä–∞—â–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∏—Ö –∏–Ω–¥–µ–∫—Å—ã –≤ —Å–ª–æ–≤–∞—Ä–µ\n",
    "* `bert_tokenizer.tokenize` - —Ä–∞–∑–±–∏—Ç–∏–µ —Ç–µ–∫—Å—Ç–∞ (–≤ —Ç–æ–º —á–∏—Å–ª–µ –∏ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤) –Ω–∞ —Ç–æ–∫–µ–Ω—ã BERT\n",
    "\n",
    "–í–∞—à–∞ –∑–∞–¥–∞—á–∞:\n",
    "1. –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π dict, –≤ –∫–æ—Ç–æ—Ä–æ–º –±—É–¥—É—Ç input_ids, attention_mask, token_type_ids\n",
    "2. –î–æ–±–∞–≤–∏—Ç—å –≤ –Ω–µ–≥–æ labels - –≤–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ NER-—Ç—ç–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç —Ä–∞–∑–±–∏—Ç—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π BERT. –î–ª—è —ç—Ç–æ–≥–æ –º–æ–∂–Ω–æ –º–æ–∂–Ω–æ —Ä–∞–∑–±–∏—Ç—å –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –æ—Ç–¥–µ–ª—å–Ω–æ –∏ —Ä–∞–∑–º–Ω–æ–∂–∏—Ç—å –µ–≥–æ –º–µ—Ç–∫–∏. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—Ñ—Ñ—Å–µ—Ç–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤ BERT, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, —á–∞—Å—Ç—å—é –∫–∞–∫–æ–≥–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∏ –∫–∞–∫–æ–π –∏—Å—Ö–æ–¥–Ω–æ–π –º–µ—Ç–∫–∏ —è–≤–ª—è–µ—Ç—Å—è –¥–∞–Ω–Ω—ã–π BERT-—Ç–æ–∫–µ–Ω."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75dd55d2-61f8-4591-8b53-999cca29ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "\n",
    "def preprocess_ner_dataset(example):\n",
    "    model_input = bert_tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "    ner_ids = [] \n",
    "\n",
    "    # –≥—Ä—É–ø–ø–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã –ø–æ –∞–π–¥–∏—à–Ω–∏–∫–∞–º —Å–ª–æ–≤\n",
    "    # –≤ group –±—É–¥—É—Ç –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞\n",
    "    for idx, group in groupby(model_input.word_ids()):\n",
    "        if idx is None:\n",
    "            ner_ids.append(-100)\n",
    "            continue\n",
    "        label = example[\"ner_tags\"][idx]\n",
    "\n",
    "        # –Ω–µ—á—ë—Ç–Ω—ã–µ –ª–µ–π–±–ª—ã –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ B-<LABEL>\n",
    "        if label % 2 == 1:\n",
    "            ner_ids.append(label)\n",
    "            next(group)  # –¥–æ–±–∞–≤–∏–ª–∏ –ª–µ–π–±–ª –∏ –ø—Ä–æ–¥–≤–∏–≥–∞–µ–º –∏—Ç–µ—Ä–∞—Ç–æ—Ä –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω\n",
    "            label += 1  # –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Ç–æ–∫–µ–Ω—ã –æ—Ç —Å–ª–æ–≤–∞ –±—É–¥—É—Ç I-<LABEL> \n",
    "\n",
    "        for part in group:\n",
    "            ner_ids.append(label)\n",
    "\n",
    "    model_input[\"labels\"] = ner_ids\n",
    "    return model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c9ed654-c4dc-4757-a000-b44bcb2f5393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003[\"train\"].features[\"ner_tags\"].feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb77fcb-4d0a-4e7d-bae8-4b1d25549e38",
   "metadata": {},
   "source": [
    "### –¢–µ—Å—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a8af22c-9da2-4bf0-a2f2-5808c72703d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_example = preprocess_ner_dataset(example)\n",
    "required_keys = [\"input_ids\", \"labels\", \"attention_mask\", \"token_type_ids\"]\n",
    "for k in required_keys:\n",
    "    assert k in processed_example, f\"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ {k}\"\n",
    "\n",
    "required_keys_set = set(required_keys)\n",
    "for k in processed_example.keys():\n",
    "    assert k in required_keys_set, f\"–í –ø—Ä–∏–º–µ—Ä–µ –ª–∏—à–Ω–µ–µ –ø–æ–ª–µ {k}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a334417-49d8-4b02-8599-0914e1474671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 773.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤–µ—Ä–Ω–∞!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for idx, example in tqdm(enumerate(conll2003[\"train\"])):\n",
    "    input_ids_real = bert_tokenizer(example[\"tokens\"], is_split_into_words=True)[\"input_ids\"]\n",
    "    input_ids_ours = preprocess_ner_dataset(example)[\"input_ids\"]\n",
    "    assert input_ids_real == input_ids_ours, f\"–û—à–∏–±–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ {idx}\"\n",
    "    if idx >= 10:\n",
    "        break\n",
    "print(\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤–µ—Ä–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bb5b4f4-3958-4421-8136-17facfd19cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = conll2003[\"train\"][100]\n",
    "processed_example = preprocess_ner_dataset(example)\n",
    "\n",
    "assert processed_example[\"labels\"][0] == -100\n",
    "assert processed_example[\"labels\"][-1] == -100\n",
    "ner_tags = [features.int2str(i) for i in processed_example[\"labels\"][1:-1]]\n",
    "assert ner_tags == ['B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c04cc2a3-7d89-4b8b-a943-7c9fe10ca561",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = conll2003[\"train\"][200]\n",
    "processed_example = preprocess_ner_dataset(example)\n",
    "\n",
    "assert processed_example[\"labels\"][0] == -100\n",
    "assert processed_example[\"labels\"][-1] == -100\n",
    "ner_tags = [features.int2str(i) for i in processed_example[\"labels\"][1:-1]]\n",
    "assert ner_tags == ['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8177dc-ab7b-4b67-8676-73c57e5b01f6",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–º–µ–Ω–∏–º –Ω–∞—à—É —Ñ—É–Ω–∫—Ü–∏—é –∫ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "106c8045-8883-46b1-ab95-0904f838a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_ner_dataset = conll2003.map(preprocess_ner_dataset, num_proc=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb4ddd-3487-4785-80a6-0f821a7e4b9a",
   "metadata": {},
   "source": [
    "–ü–æ–¥–≥–æ—Ç–æ–≤–∏–º `data_collator`. –≠—Ç–æ –æ—Å–æ–±—ã–π –∫–ª–∞—Å—Å, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∑–∞–Ω–∏–º–∞—Ç—å—Å—è –±–∞—Ç—á–µ–≤–∞–Ω–∏–µ–º —Å—ç–º–ø–ª–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –û–Ω –¥–æ–±–∞–≤–∏—Ç –ø–∞–¥–¥–∏–Ω–≥–∏ –≤–æ –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–æ–ª—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a20511a1-d222-4034-b099-9dfdd02ed81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c7c640-ac2b-4388-81b6-9f21c8c2ec2d",
   "metadata": {},
   "source": [
    "### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ (5 –±–∞–ª–ª–æ–≤)\n",
    "\n",
    "–î–≤–∞ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—É—Ç–∏ –Ω–∞ —ç—Ç–æ–π —Å—Ç–∞–¥–∏–∏:\n",
    "1. –í–∑—è—Ç—å [–≥–æ—Ç–æ–≤—ã–π –∫–ª–∞—Å—Å](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelfortokenclassification) –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤. (–≠—Ç–æ—Ç –≤–∞—Ä–∏–∞–Ω—Ç –Ω–∞—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)\n",
    "2. –í–∑—è—Ç—å –º–æ–¥–µ–ª—å –∫–∞–∫ —Ñ–∏—á–∞ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä ([AutoModel](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodel)) –∏ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—É—é –≥–æ–ª–æ–≤—É. –í–¥–æ—Ö–Ω–æ–≤–∏—Ç—å—Å—è –º–æ–∂–Ω–æ –ø–æ [—Å—Å—ã–ª–∫–µ](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py#L1847-L1860).\n",
    "\n",
    "–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª–æ–≥–∏—Ç—ã/–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è `conll2003[\"train\"].features[\"ner_tags\"].feature.num_classes` –∫–ª–∞—Å—Å–æ–≤.\n",
    "\n",
    "> –ï—Å–ª–∏ –≤—ã–±–µ—Ä–µ—Ç–µ –≤–∞—Ä–∏–∞–Ω—Ç –Ω–æ–º–µ—Ä –æ–¥–∏–Ω, –æ–ø–∏—à–∏—Ç–µ –∫–∞–∫ –æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç - –∫–∞–∫ –∏–∑ —Ç–æ–∫–µ–Ω–∞ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –µ–≥–æ `ner_tag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "331cd38a-3501-4f41-9193-8cf6579ad80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoModel\n",
    "\n",
    "\n",
    "label_names = conll2003[\"train\"].features[\"ner_tags\"].feature.names\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "bert_ner = AutoModelForTokenClassification.from_pretrained(\n",
    "    BASE_NER_MODEL, \n",
    "    num_labels=conll2003[\"train\"].features[\"ner_tags\"].feature.num_classes,\n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eaa2ce-0f56-4b8f-8829-ff9edffa0d34",
   "metadata": {},
   "source": [
    "### –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –ú–µ—Ç—Ä–∏–∫—É (5 –±–∞–ª–ª–æ–≤)\n",
    "\n",
    "–î–æ–ø–æ–ª–Ω–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é, –∏—Å–ø–æ–ª—å–∑—É—è `metrics_calculator`, —á—Ç–æ–±—ã –æ–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞–ª–∞ `accuracy`, `precision`, `recall` –∏ `f-–º–µ—Ä—É`. `eval_predictions` - —ç—Ç–æ –∫–æ—Ä—Ç–µ–∂ –∏–∑ –ª–æ–≥–∏—Ç–æ–≤ —Ç–æ–∫–µ–Ω –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∏ `labels`, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∏ —Å –ø–æ–º–æ—â—å—é `preprocess_ner_dataset`. –ù—É–∂–Ω–æ\n",
    "1. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –ª–æ–≥–∏—Ç—ã –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –ª–µ–π–±–ª—ã. –£—á—Ç–∏—Ç–µ, —á—Ç–æ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –ª–µ–π–±–ª–æ–≤ –Ω–µ—Ç\n",
    "2. –ü–æ—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ —Å –ø–æ–º–æ—â—å—é `metrics_calculator`\n",
    "3. –£–ø–∞–∫–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—Ç–∞—Ç –≤ `dict`, –≤ –∫–æ—Ç–æ—Ä–æ–º –∫–ª—é—á—ë–º –±—É–¥–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏, –∞ –∑–Ω–∞—á–µ–Ω–∏–µ–º - –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "\n",
    "–í logits –±—É–¥–µ—Ç –ª–µ–∂–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ \\[—Ä–∞–∑–º–µ—Ä eval –¥–∞—Ç–∞—Å–µ—Ç–∞, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —á–∏—Å–ª–æ –º–µ—Ç–æ–∫\\], —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "\n",
    "–í target_labels –±—É–¥–µ—Ç –ª–µ–∂–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ \\[—Ä–∞–∑–º–µ—Ä eval –¥–∞—Ç–∞—Å–µ—Ç–∞, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\\], —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –º–µ—Ç–∫–∏ –∏–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏.\n",
    "\n",
    "–ü—Ä–∏–º–µ—Ä—ã —Ñ—É–Ω–∫—Ü–∏–∏ calculate_metrics –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –≤ [–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏](https://huggingface.co/docs/evaluate/en/transformers_integrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55ad18b4-5176-4bf1-ab9a-517466e79f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "\n",
    "metrics_calculator = evaluate.load(\"seqeval\")\n",
    "label_list = conll2003[\"train\"].features[\"ner_tags\"].feature.names\n",
    "\n",
    "\n",
    "def calculate_metrics(eval_predictions):\n",
    "    logits, labels = eval_predictions\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    filtered_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    filtered_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    metrics = metrics_calculator.compute(predictions=filtered_predictions, references=filtered_labels)\n",
    "    return {\n",
    "        name: value for name, value in metrics.items() if isinstance(value, float)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44528ae-e63a-4561-b707-a891f8dcf75f",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ (5 –±–∞–ª–ª–æ–≤)\n",
    "\n",
    "–î–≤–∞ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—É—Ç–∏ –Ω–∞ —ç—Ç–æ–π —Å—Ç–∞–¥–∏–∏:\n",
    "\n",
    "1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å [Trainer](https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html) –∫–ª–∞—Å—Å –∏–∑ `transformers`\n",
    "2. –ù–∞–ø–∏—Å–∞—Ç—å —Å–≤–æ–π training loop\n",
    "\n",
    "–û–ø–∏—à–µ–º –ø–æ–¥—Ä–æ–±–Ω–µ–µ –ø–µ—Ä–≤—ã–π –ø—É—Ç—å, —Ç.–∫. –æ–Ω –Ω–∞—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è.\n",
    "\n",
    "–ù—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –∫–ª–∞—Å—Å Trainer –∏ TrainingArguments.\n",
    "–í [TrainingArguments](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments) –Ω—É–∂–Ω–æ –∫–∞–∫ –º–∏–Ω–∏–º—É–º —Å–ª–µ–¥—É—é—â–∏–µ –ø–æ–ª—è:\n",
    "* save_strategy, eval_strategy\n",
    "* metric_for_best_model (–∏—Å—Ö–æ–¥—è –∏–∑ calculate_metrics), greater_is_better\n",
    "* learning_rate (–≤–æ–∑—å–º–∏—Ç–µ 2e-5)\n",
    "* num_train_epochs\n",
    "* per_device_train_batch_size, per_device_eval_batch_size\n",
    "\n",
    "–í –∫–ª–∞—Å—Å Trainer –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å:\n",
    "* model\n",
    "* –≤ args –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ TrainingArguments\n",
    "* train_dataset, eval_dataset\n",
    "* tokenizer\n",
    "* compute_metrics\n",
    "\n",
    "–ü–æ—Å–ª–µ —á–µ–≥–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å `trainer.train()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3593874-6c27-464d-a424-603f7c4ab4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [370/370 03:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.310191</td>\n",
       "      <td>0.540816</td>\n",
       "      <td>0.624369</td>\n",
       "      <td>0.579597</td>\n",
       "      <td>0.907120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.115728</td>\n",
       "      <td>0.795829</td>\n",
       "      <td>0.854090</td>\n",
       "      <td>0.823931</td>\n",
       "      <td>0.966180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.078459</td>\n",
       "      <td>0.852978</td>\n",
       "      <td>0.906092</td>\n",
       "      <td>0.878733</td>\n",
       "      <td>0.978439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.068223</td>\n",
       "      <td>0.891033</td>\n",
       "      <td>0.924773</td>\n",
       "      <td>0.907589</td>\n",
       "      <td>0.982251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.063195</td>\n",
       "      <td>0.898492</td>\n",
       "      <td>0.932514</td>\n",
       "      <td>0.915187</td>\n",
       "      <td>0.982987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.062551</td>\n",
       "      <td>0.906901</td>\n",
       "      <td>0.937731</td>\n",
       "      <td>0.922059</td>\n",
       "      <td>0.983958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.061926</td>\n",
       "      <td>0.913768</td>\n",
       "      <td>0.941602</td>\n",
       "      <td>0.927476</td>\n",
       "      <td>0.984473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.061698</td>\n",
       "      <td>0.918089</td>\n",
       "      <td>0.941266</td>\n",
       "      <td>0.929533</td>\n",
       "      <td>0.984738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.061941</td>\n",
       "      <td>0.913356</td>\n",
       "      <td>0.940256</td>\n",
       "      <td>0.926611</td>\n",
       "      <td>0.984532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.061676</td>\n",
       "      <td>0.917445</td>\n",
       "      <td>0.942612</td>\n",
       "      <td>0.929858</td>\n",
       "      <td>0.985047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=370, training_loss=0.14148366773450696, metrics={'train_runtime': 233.158, 'train_samples_per_second': 602.21, 'train_steps_per_second': 1.587, 'total_flos': 5288732502779718.0, 'train_loss': 0.14148366773450696, 'epoch': 10.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"bert_ner_conll\",\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=256,  # –ø–æ—á–µ–º—É –≤–æ –≤—Å–µ—Ö —Å–¥–∞–Ω—ã—Ö –¥–æ–º–∞—à–∫–∞—Ö —ç—Ç–æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä —Ä–∞–≤–µ–Ω train_batch_size?\n",
    "    fp16=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert_ner,\n",
    "    tokenizer=bert_tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=preprocessed_ner_dataset[\"train\"],\n",
    "    eval_dataset=preprocessed_ner_dataset[\"validation\"],\n",
    "    compute_metrics=calculate_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe8a152-f724-4c8d-9949-74e346b6a842",
   "metadata": {},
   "source": [
    "### –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (5 –±–∞–ª–ª–æ–≤)\n",
    "\n",
    "–ü–æ–¥—É–º–∞—Ç—å –æ:\n",
    "1. –í–æ –≤—Ä–µ–º—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –º—ã –ø—Ä–∏–æ–±—Ä–∞–∑–æ–≤–∞–ª–∏ BIO —Ä–∞–∑–º–µ—Ç–∫—É. –ö–∞–∫ –æ–±—Ä–∞—Ç–∏—Ç—å —ç—Ç–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞?\n",
    "\n",
    "–ü—Ä–æ–≤–∞–ª–∏–¥–∏—Ä—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9227c6ba-6b7b-4a18-8502-cd225c0d2dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.14958275854587555,\n",
       " 'test_overall_precision': 0.869757174392936,\n",
       " 'test_overall_recall': 0.9068696883852692,\n",
       " 'test_overall_f1': 0.8879258039351652,\n",
       " 'test_overall_accuracy': 0.9695876207434487,\n",
       " 'test_runtime': 2.9298,\n",
       " 'test_samples_per_second': 1178.587,\n",
       " 'test_steps_per_second': 1.707}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "*_, metrics = trainer.predict(preprocessed_ner_dataset['test'])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fda7c62-f8eb-4a83-9659-71ef3a8610fb",
   "metadata": {},
   "source": [
    "–ú–æ–∂–µ—Ç–µ —Å—Ä–∞–≤–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å [–ª–∏–¥–µ—Ä–±–æ—Ä–¥–æ–º](https://paperswithcode.com/sota/token-classification-on-conll2003).\n",
    "\n",
    "\n",
    "–ù–∞–ø–∏—à–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ —Ç–µ–∫—Å—Ç –∏ –æ—Ç–¥–∞—ë—Ç —Ç–∞–∫–æ–π —Å–ª–æ–≤–∞—Ä—å:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"text\": \"–≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç\",\n",
    "    \"entities\": [\n",
    "        {\n",
    "            \"class\": \"–ª–µ–π–±–ª –∫–ª–∞—Å—Å–∞\",\n",
    "            \"text\": \"—Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ\",\n",
    "            \"start\": \"–æ—Ñ—Ñ—Å–µ—Ç –æ—Ç –Ω–∞—á–∞–ª–∞ —Å—Ç—Ä–æ–∫–∏ –¥–æ –Ω–∞—á–∞–ª–∞ entity\",\n",
    "            \"end\": \"–æ—Ñ—Ñ—Å–µ—Ç –æ—Ç –Ω–∞—á–∞–ª–∞ —Å—Ç—Ä–æ–∫–∏ –¥–æ –∫–æ–Ω—Ü–∞ entity\"\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "–î–æ–ª–∂–Ω–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è —Ç–∞–∫–æ–µ —É—Å–ª–æ–≤–∏–µ:\n",
    "\n",
    "```python\n",
    "text[entity[\"start\"]:entity[\"stop\"]] == entity[\"text\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c8f48b4-932d-4525-b8d7-8b57f324520d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Ivan Petrov is going to start working tomorrow', 'entities': [{'class': 'PER', 'start': 0, 'end': 11, 'text': 'Ivan Petrov'}]}\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad\n",
    "def do_ner(text):\n",
    "    bert_ner.eval()\n",
    "    \n",
    "    tokenized = bert_tokenizer(text, return_offsets_mapping=True, return_tensors='pt').to(bert_ner.device)\n",
    "    offsets = tokenized['offset_mapping'].cpu().numpy()[0]\n",
    "\n",
    "    logits = bert_ner(tokenized.input_ids).logits\n",
    "    preds = torch.argmax(logits, dim=-1).cpu().numpy()[0]\n",
    "    \n",
    "    \n",
    "    entities = []\n",
    "    for word_id, pred, offset in zip(tokenized.word_ids(), preds, offsets):\n",
    "        if word_id is None or pred == 0:\n",
    "            continue\n",
    "    \n",
    "        if pred % 2 == 1:\n",
    "            # –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è —Å—É—â–Ω–æ—Å—Ç—å\n",
    "            entities.append(\n",
    "                {\n",
    "                    \"start\": offset[0],\n",
    "                    \"end\": offset[1],\n",
    "                    \"class\": label_list[pred].split(\"-\")[-1],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            # –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å—É—â–Ω–æ—Å—Ç—å\n",
    "            entities[-1][\"end\"] = offset[1]\n",
    "\n",
    "    return {\n",
    "        \"text\": text, \n",
    "        \"entities\": [\n",
    "            {\n",
    "                \"class\": entity[\"class\"],\n",
    "                \"start\": entity[\"start\"],\n",
    "                \"end\": entity[\"end\"],\n",
    "                \"text\": text[entity[\"start\"]:entity[\"end\"]],\n",
    "            } for entity in entities\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "print(do_ner(\"Ivan Petrov is going to start working tomorrow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a22542f-17e7-4a33-8dc4-f5b94611fa1f",
   "metadata": {},
   "source": [
    "–ü–æ—á–∏—Å—Ç–∏–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –≤—Ç–æ—Ä–æ–π —á–∞—Å—Ç—å—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7516c7bb-88ac-4d60-a6a0-4cfebd6fa0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "del bert_ner\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a06a5cc-44c3-41f9-ba9e-a2a5f5b1e04b",
   "metadata": {},
   "source": [
    "## –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å T5 (25 –±–∞–ª–ª–æ–≤)\n",
    "\n",
    "–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–æ–±—É—á–∏—Ç—å [t5-small](https://huggingface.co/google-t5/t5-small) –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–∫—Å–∏—á–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –∏–∑ [—ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞](https://huggingface.co/datasets/lmsys/toxic-chat). –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–æ–ª–∂–µ–Ω —Ä–∞–±–æ—Ç–∞—Ç—å –≤ —Å—Ç–∏–ª–µ t5 - –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Ç–µ–∫—Å—Ç–æ–º.\n",
    "\n",
    "1. –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "\t1. –ü—Ä–∏–¥—É–º–∞—Ç—å –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –∑–∞–¥–∞—á–∏ –∏–ª–∏ –≤–∑—è—Ç—å –∏–∑ –ø–æ—Ö–æ–∂–µ–π –º–æ–¥–µ–ª–∏\n",
    "\t2. –í—ã–±—Ä–∞—Ç—å —Ç–æ–∫–µ–Ω—ã –¥–ª—è –∫–ª–∞—Å—Å–æ–≤\n",
    "2. –û–±—É—á–∏—Ç—å t5-small –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞\n",
    "3. –°—Ä–∞–≤–Ω–∏—Ç—å —Å –º–æ–¥–µ–ª—å —Å –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–π –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c536e268-d5c0-461c-bdc1-553e51b1ce66",
   "metadata": {},
   "source": [
    "### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –î–∞–Ω–Ω—ã—Ö (6 –±–∞–ª–ª–æ–≤)\n",
    "\n",
    "–ü–æ–¥—É–º–∞—Ç—å –æ:\n",
    "1. –ö–∞–∫–æ–π –ø—Ä–µ—Ñ–∏–∫—Å –≤—ã–±—Ä–∞—Ç—å –¥–ª—è –Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏?\n",
    "2. –î–æ–ª–∂–µ–Ω –ª–∏ –ø—Ä–µ—Ñ–∏–∫—Å –±—ã—Ç—å –ø–æ–Ω—è—Ç–Ω—ã–º?\n",
    "3. –ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å –º–µ—Ç–∫—É –¥–ª—è –∫–ª–∞—Å—Å–∞? \n",
    "4. –ß—Ç–æ –±—É–¥–µ—Ç, –µ—Å–ª–∏ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–∞ —Ü–µ–ª–∏–∫–æ–º –Ω–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ?\n",
    "5. –ß—Ç–æ –¥–µ–ª–∞—Ç—å —Å –¥–ª–∏–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏?\n",
    "\n",
    "–î–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∫ LLM –∏ —Ä–∞–∑–º–µ—Ç–∫—É, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å —Ç–æ–∫—Å–∏—á–Ω—ã–º. –ù–∞—Å –±—É–¥—É—Ç –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏ `\"user_input\"` –∏ `\"toxicity\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fb5e0e5-677d-4084-8a68-7241ac05982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "BASE_T5_MODEL= \"t5-small\"\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(BASE_T5_MODEL)\n",
    "\n",
    "\n",
    "toxic_chat_dataset = load_dataset(\"lmsys/toxic-chat\", \"toxicchat0124\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec97b3aa-4d0a-4b87-bd62-f0c2de297f9f",
   "metadata": {},
   "source": [
    "–ú–µ—Å—Ç–æ –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96556315-dfef-4ea3-b965-4fa4191d4d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv_id': 'e0c9b3e05414814485dbdcb9a29334d502e59803af9c26df03e9d1de5e7afe67',\n",
       " 'user_input': 'Masturbacja jest proces co oitrzebuje',\n",
       " 'model_output': 'Masturbacja to proces, kt√≥ry mo≈ºe pozytywnie wp≈ÇynƒÖƒá na zdrowie psychiczne i fizyczne cz≈Çowieka, poniewa≈º pomaga w relaksie, redukuje stres i pomaga w u≈õpieniu. Mo≈ºe te≈º byƒá u≈ºywana jako dodatkowa form',\n",
       " 'human_annotation': True,\n",
       " 'toxicity': 0,\n",
       " 'jailbreaking': 0,\n",
       " 'openai_moderation': '[[\"sexual\", 0.4609803557395935], [\"sexual/minors\", 0.0012527990620583296], [\"harassment\", 0.0001862536446424201], [\"hate\", 0.00015521160094067454], [\"violence\", 6.580814078915864e-05], [\"self-harm\", 3.212967567378655e-05], [\"violence/graphic\", 1.5190824342425913e-05], [\"self-harm/instructions\", 1.0009921425080393e-05], [\"hate/threatening\", 4.4459093260229565e-06], [\"self-harm/intent\", 3.378846486157272e-06], [\"harassment/threatening\", 1.7095695739044459e-06]]'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_chat_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b54e8e06-c538-4f50-b31c-c99451c1143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_chat_dataset = toxic_chat_dataset.remove_columns(\n",
    "    [\"conv_id\", \"model_output\", \"human_annotation\", \"jailbreaking\", \"openai_moderation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ccfb9-3ddc-4a7a-876f-44dacace228d",
   "metadata": {},
   "source": [
    "![](https://production-media.paperswithcode.com/methods/new_text_to_text.jpg)\n",
    "\n",
    "–í—ã–±–µ—Ä–µ—Ç–µ `PREFIX` –¥–ª—è –∑–∞–¥–∞—á–∏, –ª–µ–π–±–ª—ã –¥–ª—è –¥–≤—É—Ö –∫–ª–∞—Å—Å–æ–≤ –∏ –Ω–∞–ø–∏—à–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏. –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ –µ—Å—Ç—å –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ –≤—ã—à–µ - `translate English to German` –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ `summarize` –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏. –í –∫–∞—á–µ—Å—Ç–≤–µ –ª–µ–π–±–ª–æ–≤ —É –≤–∞—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –æ–±–æ–∑–Ω–∞—á–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å. –≠—Ç–æ—Ç —Ç–µ–∫—Å—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –ª—é–±–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, –æ—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ `\"–¥–∞\"/\"–Ω–µ—Ç\"`, –¥–æ `\"–û—Ç —ç—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤–µ–µ—Ç —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å—é\"/\"–¶–µ–Ω–∑—É—Ä–∞ —Å–ø–æ–∫–æ–π–Ω–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç –¥–∞–ª—å—à–µ\"`. –ü–æ–¥—É–º–∞–π—Ç–µ –≤ —á—ë–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –ø–µ—Ä–≤–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –ø–µ—Ä–µ–¥ –≤—Ç–æ—Ä—ã–º.\n",
    "\n",
    "–í–∞–∂–Ω–æ:\n",
    "1) –ù–µ –∑–∞–±—ã—Ç—å –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–µ—Ñ–∏–∫—Å –ø–µ—Ä–µ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "2) –õ–µ–π–±–ª–∞–º–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –≤—ã—Å—Ç—É–ø–∞—é—Ç —É–∂–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –æ–∂–∏–¥–∞–µ–º –Ω–∞ –≤—ã—Ö–æ–¥–µ –∏–∑ –¥–µ–∫–æ–¥–µ—Ä–∞\n",
    "\n",
    "–¢–µ–∫—Å—Ç –≤ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –º–æ–∂–Ω–æ –ø–æ–¥–∞–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏:\n",
    "1. `tokenizer(text=\"text\")` - —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∫–∞–∫ –æ–±—ã—á–Ω–æ\n",
    "1. `tokenizer(text_target=\"text\")` - —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–π —ç—Ç–æ –∫–∞–∫ —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –º—ã –æ–∂–∏–¥–∞–µ–º —É–≤–∏–¥–µ—Ç—å –Ω–∞ –≤—ã—Ö–æ–¥–µ –∏–∑ –¥–µ–∫–æ–¥–µ—Ä–∞. –í —Å–ª—É—á–∞–µ t5 —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ —Ä–∞–∑–Ω–∏—Ü—ã –Ω–µ—Ç, –Ω–æ –¥–ª—è –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ —Ç–∞–∫\n",
    "1. –î—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã –º–æ–∂–Ω–æ —É–∑–Ω–∞—Ç—å –ø–æ—Å–º–æ—Ç—Ä–µ–≤ —Å–∏–≥–Ω–∞—Ç—É—Ä—É –º–µ—Ç–æ–¥–∞ `tokenizer.__call__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ecb8468-d79b-4b05-baed-d6730e50e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?t5_tokenizer.__call__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f11fe1-ebed-4c61-9415-5a48c5f5bf1d",
   "metadata": {},
   "source": [
    "–í–∞–∂–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã:\n",
    "1. –ü—Ä–µ—Ñ–∏–∫—Å –º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –ª—é–±–æ–π, —Ö–æ—Ç—å —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π, —Ö–æ—Ç—å –∞–±—Å–æ–ª—é—Ç–Ω–æ –Ω–µ–ø–æ–Ω—è—Ç–Ω—ã–π - –ø—Ä–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ –¥–ª—è –º–æ–¥–µ–ª–∏ —Ä–∞–∑–Ω–∏—Ü—ã –Ω–µ –±—É–¥–µ—Ç. –ù–æ –≤–∞–∂–Ω–æ —É—á–µ—Å—Ç—å, —á—Ç–æ —á–µ–º –¥–ª–∏–Ω–Ω–µ–µ –ø—Ä–µ—Ñ–∏–∫—Å (–≤ —Ç–æ–∫–µ–Ω–∞—Ö), —Ç–µ–º –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –≤—ã —Ç—Ä–∞—Ç–∏—Ç–µ –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ, —Ç–∞–∫ –∫–∞–∫ –µ–≥–æ –ø—Ä–∏–¥—ë—Ç—Å—è –¥–æ–±–∞–≤–ª—è—Ç—å –∫ —Ç–æ–º—É —Ç–µ–∫—Å—Ç—É, –∫–æ—Ç–æ—Ä—ã–π –≤—ã —Ö–æ—Ç–∏—Ç–µ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å\n",
    "2. –õ–µ–π–±–ª—ã —Ç–æ–∂–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–∞–∫–∏–º–∏ —É–≥–æ–¥–Ω–æ. –ù–æ –≤ –∏—Ö —Å–ª—É—á–∞–µ –¥–ª–∏–Ω–Ω–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö –∏–≥—Ä–∞–µ—Ç –µ—â—ë –±–æ–ª—å—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —Ç–∞–∫ –∫–∞–∫ –≤ \"–ø—Ä–æ–¥–µ\" –ø—Ä–∏–¥—ë—Ç—Å—è –¥–µ–ª–∞—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –¥–µ–∫–æ–¥–µ—Ä–Ω–æ–π —á–∞—Å—Ç–∏ —Å—Ç–æ–ª—å–∫–æ —Ä–∞–∑, —Å–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –≤–∞—à–µ–º –ª–µ–π–±–ª–µ + 1 —Å—Ç–æ–ø —Ç–æ–∫–µ–Ω (`</s>`). –ï—Å–ª–∏ –æ–±–∞ –ª–µ–π–±–ª–∞ –±—É–¥—É—Ç –¥–ª–∏–Ω–Ω—ã 1, —Ç–æ –µ—â—ë –∏ accuracy –±—É–¥–µ—Ç –ø—Ä–æ—â–µ —Å—á–∏—Ç–∞—Ç—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bda531e9-c017-4269-8a0d-4bd9b36bacb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [12068, 10], 'attention_mask': [1, 1]}\n",
      "{'input_ids': [150], 'attention_mask': [1]}\n",
      "{'input_ids': [4273], 'attention_mask': [1]}\n"
     ]
    }
   ],
   "source": [
    "for text in (\"toxic: \", \"no\", \"yes\"):\n",
    "    print(t5_tokenizer(text, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab7b378-6a86-4cb6-ae4f-b40e54bd0b2b",
   "metadata": {},
   "source": [
    "–ü—Ä–µ—Ñ–∏–∫—Å –¥–æ–±–∞–≤–ª—è–µ—Ç 2 —Ç–æ–∫–µ–Ω–∞ (—Ç–µ—Ä–ø–∏–º–æ), –ª–µ–π–±–ª—ã –≤—Å–µ –ø–æ –æ–¥–Ω–æ–º—É —Ç–æ–∫–µ–Ω—É - –∑–Ω–∞—á–∏—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –¥–µ–∫–æ–¥–µ—Ä–∞ –≤ –ø—Ä–æ–¥–µ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –¥–µ–ª–∞—Ç—å –¥–≤–∞ —Ä–∞–∑–∞ (–≤ —Ç–µ–æ—Ä–∏–∏ –º–æ–∂–Ω–æ –¥–∞–∂–µ –æ–¥–Ω–∏–º –æ–±–æ–π—Ç–∏—Å—å).\n",
    "\n",
    "–í —Å–¥–∞–Ω–Ω—ã—Ö –¥–æ–º–∞—à–∫–∞—Ö –º–æ–∂–Ω–æ –±—ã–ª–æ —á–∞—Å—Ç–æ —É–≤–∏–¥–µ—Ç—å –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –Ω–µ–¥–æ—á—ë—Ç:\n",
    "1. –ü–∞–¥–¥–∏–Ω–≥ –≤—Ö–æ–¥–∞ –≤ —ç–Ω–∫–æ–¥–µ—Ä\n",
    "2. –ü–∞–¥–¥–∏–Ω–≥ –≤—Ö–æ–¥–∞ –≤ –¥–µ–∫–æ–¥–µ—Ä\n",
    "\n",
    "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –≤—Å–µ–≥–æ –¥–µ–ª–∞—Ç—å –ø–∞–¥–¥–∏–Ω–≥ –Ω–∞ —ç—Ç–∞–ø–µ —Å–±–æ—Ä–∫–∏ –±–∞—Ç—á–∞, —Ç–∞–∫ –∫–∞–∫ —Ç–æ–≥–¥–∞ —Ç–æ—á–Ω–æ –Ω–µ –ø–æ–ª—É—á–∏—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –ª–∏—à–Ω–µ–≥–æ. –ü–∞–¥–¥–∏–Ω–≥ –≤—Ö–æ–¥–∞ –≤ –¥–µ–∫–æ–¥–µ—Ä –≤–æ–æ–±—â–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∑–∞ —Å—á—ë—Ç –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ª–µ–π–±–ª–æ–≤ - —Ç–∞–º –≤—Å–µ–≥–¥–∞ –±—É–¥–µ—Ç –≤—Ö–æ–¥ —Ä–∞–∑–º–µ—Ä–∞ 2.\n",
    "\n",
    "`MAX_LENGTH` —Ç—É—Ç –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –Ω–µ –¥–ª—è –ø–∞–¥–¥–∏–Ω–≥–æ–≤, –∞ –¥–ª—è truncation - –µ—Å–ª–∏ –µ–≥–æ –Ω–µ –¥–µ–ª–∞—Ç—å, —Ç–æ –º–æ–∂–Ω–æ  —É–≤–∏–¥–µ—Ç—å warning:\n",
    "> Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f018fae1-bf63-4e62-be55-2454ce8d1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"toxic: \"\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "id2label = {\n",
    "    0: \"no\",\n",
    "    1: \"yes\",\n",
    "}\n",
    "\n",
    "\n",
    "def preprocess_dataset(example):\n",
    "    input_texts = PREFIX + example[\"user_input\"]\n",
    "    model_inputs = t5_tokenizer(input_texts, truncation=True, max_length=MAX_LENGTH)\n",
    "    model_inputs[\"labels\"] = t5_tokenizer(id2label[example[\"toxicity\"]]).input_ids\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "toxic_chat_dataset = toxic_chat_dataset.map(preprocess_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880cd560-829c-4ad4-830b-e5e21323365b",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞:\n",
    "```json\n",
    "{'user_input': 'Do you know drug which name is abexol ?',\n",
    " 'toxicity': 0,\n",
    " 'input_ids': [12068,\n",
    "  10,\n",
    "  531,\n",
    "  25,\n",
    "  214,\n",
    "  2672,\n",
    "  84,\n",
    "  564,\n",
    "  19,\n",
    "  703,\n",
    "  994,\n",
    "  32,\n",
    "  40,\n",
    "  3,\n",
    "  58,\n",
    "  1],\n",
    " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    " 'labels': [150, 1]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe2b7fee-e975-4ad2-a02f-f0a00c87aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_chat_dataset = toxic_chat_dataset.remove_columns(\"user_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef085f1-2adf-41da-9f37-dded6aecc3b4",
   "metadata": {},
   "source": [
    "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∑–∞–¥–∞—á–µ `DataCollator`. –ò–¥—ë–º —Ç–µ–º –∂–µ –∫—É—Ä—Å–æ–º, —á—Ç–æ –∏ –≤ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏ –¥–æ–º–∞—à–∫–∏, –æ–¥–Ω–∞–∫–æ —ç—Ç–æ—Ç –¥–∞—Ç–∞ –∫–æ–ª–ª–∞—Ç–æ—Ä —É–∂–µ –Ω–µ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ –º–æ–¥–µ–ª–∏. –ü–æ—á–µ–º—É —Ç–∞–∫?\n",
    "\n",
    "–í—Å—ë –¥–µ–ª–æ –≤ —Ñ—É–Ω–∫—Ü–∏–∏ `seq2seq_model.prepare_decoder_input_ids_from_labels`, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ª–µ–π–±–ª—ã –≤–æ –≤—Ö–æ–¥ –¥–µ–∫–æ–¥–µ—Ä–∞. –û—Ç–ª–æ–∂–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞ –∫–æ–ª–ª–∞—Ç–æ—Ä–∞ –¥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fba319be-4d46-4f20-af5a-be137cea178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer=t5_tokenizer, model=seq2seq_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb5f662-23a7-4285-bc93-a9cd9bce7fbc",
   "metadata": {},
   "source": [
    "### –û–ø—Ä–µ–¥–µ–ª–∏–º –º–µ—Ç—Ä–∏–∫—É (2 –±–∞–ª–ª–∞)\n",
    "\n",
    "–í —ç—Ç–æ–π –∑–∞–¥–∞—á–µ –º–µ—Ç—Ä–∏–∫–∞ –ø—Ä–æ—Å—Ç–∞—è - `accuracy`. –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∂–µ–ª–∞–Ω–∏—é.\n",
    "\n",
    "–†–∞–Ω–µ–µ —è —É–ø–æ–º–∏–Ω–∞–ª, —á—Ç–æ –æ–¥–Ω–æ—Ç–æ–∫–µ–Ω–Ω—ã–µ –ª–µ–π–±–ª—ã —ç—Ç–æ —Ö–æ—Ä–æ—à–æ. –ü–æ–∑–≤–æ–ª—è–µ—Ç —Å–¥–µ–ª–∞—Ç—å –≤–æ—Ç —Ç–∞–∫—É—é –∫–æ–º–ø–∞–∫—Ç–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é `compute_metric`. –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ö–æ—Ç—å 100 —Ç–æ–∫–µ–Ω–æ–≤, –º–µ–Ω—è –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç —Ç–æ–ª—å–∫–æ —Ç–æ—Ç, –∫–æ—Ç–æ—Ä—ã–π —è –±—ã —Ö–æ—Ç–µ–ª –Ω–∞ –º–µ—Å—Ç–µ –ª–µ–π–±–ª–∞. –¢–∞–∫ –∫–∞–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ —è –¥–µ–ª–∞—é —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø–æ –æ–¥–Ω–æ–º—É —Ç–æ–∫–µ–Ω—É, —è –º–æ–≥—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `.mean()`. –ï—Å–ª–∏ –±—ã —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –±—ã–ª–æ –±–æ–ª—å—à–µ, —Ç–∞–∫ –ø—Ä–æ—Å—Ç–æ —è –±—ã –Ω–µ –æ—Ç–¥–µ–ª–∞–ª—Å—è - –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –¥–æ–ª–∂–Ω—ã –±—ã–ª–∏ —Å–æ–≤–ø–∞—Å—Ç—å –¥–ª—è –≤–µ—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "692e79ae-1921-4853-9355-ed851bdbc893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(eval_predictions):\n",
    "    preds, labels = eval_predictions\n",
    "    return {\"accuracy\": (preds[:, 1] == labels[:, 0]).mean()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b467fa6-a3e5-40c2-97dc-777bf351a405",
   "metadata": {},
   "source": [
    "### –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ú–æ–¥–µ–ª—å (2 –±–∞–ª–ª–∞)\n",
    "\n",
    "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –º–æ–¥–µ–ª—å –∏–∑ –±–∞–∑–æ–≤–æ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞.\n",
    "\n",
    "–ù–æ —Å–Ω–∞—á–∞–ª–∞, –º—ã –º–æ–∂–µ–º –¥–æ–±–∞–≤–∏—Ç—å –≤ –∫–æ–Ω—Ñ–∏–≥ –º–æ–¥–µ–ª–∏ –Ω–∞—à—É —Ç–∞—Å–∫—É:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60e93bdc-744d-45f3-b561-42141694e8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summarization': {'early_stopping': True,\n",
       "  'length_penalty': 2.0,\n",
       "  'max_length': 200,\n",
       "  'min_length': 30,\n",
       "  'no_repeat_ngram_size': 3,\n",
       "  'num_beams': 4,\n",
       "  'prefix': 'summarize: '},\n",
       " 'translation_en_to_de': {'early_stopping': True,\n",
       "  'max_length': 300,\n",
       "  'num_beams': 4,\n",
       "  'prefix': 'translate English to German: '},\n",
       " 'translation_en_to_fr': {'early_stopping': True,\n",
       "  'max_length': 300,\n",
       "  'num_beams': 4,\n",
       "  'prefix': 'translate English to French: '},\n",
       " 'translation_en_to_ro': {'early_stopping': True,\n",
       "  'max_length': 300,\n",
       "  'num_beams': 4,\n",
       "  'prefix': 'translate English to Romanian: '}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Config\n",
    "\n",
    "\n",
    "t5_config = T5Config.from_pretrained(BASE_T5_MODEL)\n",
    "t5_config.task_specific_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eac15c3d-85c8-4a1a-81c3-4261aef42654",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"toxic_classification\"\n",
    "\n",
    "t5_config.task_specific_params[task_name] = {\n",
    "    \"prefix\": PREFIX,\n",
    "    \"max_length\": 3,\n",
    "    \"min_length\": 3,\n",
    "    \"num_beams\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc149900-fcdb-4924-8228-014083c46b53",
   "metadata": {},
   "source": [
    "–≠—Ç–æ—Ç –∫–æ–Ω—Ñ–∏–≥ –ø–æ—Ç–æ–º –º–æ–∂–Ω–æ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ pipeline –∫–ª–∞—Å—Å–µ, –ø–µ—Ä–µ–¥–∞–≤ –∞—Ä–≥—É–º–µ–Ω—Ç [task](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.Text2TextGenerationPipeline.task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61c867ec-6c8d-497c-848f-37bfb274eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, T5Config\n",
    "\n",
    "\n",
    "seq2seq_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_T5_MODEL, config=t5_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7881ca49-322b-4dde-80f6-7b431cbf89b9",
   "metadata": {},
   "source": [
    "–í—Å–ø–æ–º–∏–Ω–∞–µ–º –ø—Ä–æ –∫–æ–ª–ª–∞—Ç–æ—Ä!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d4f6e5c-68e2-4494-8055-5ae481aa9c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=t5_tokenizer, \n",
    "    model=seq2seq_model, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314bf460-ca11-4b2e-8d54-1c6317b001fa",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ (10 –±–∞–ª–ª–æ–≤)\n",
    "\n",
    "–î–≤–∞ –ø—É—Ç–∏:\n",
    "1) –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–π `Seq2SeqTrainer` –∫–ª–∞—Å—Å –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\n",
    "2) –ù–∞–ø–∏—Å–∞—Ç—å —Å–≤–æ–π training loop\n",
    "\n",
    "> Hint! –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Ñ—É–Ω–∫—Ü–∏—é `seq2seq_model.prepare_decoder_input_ids_from_labels` –µ—Å–ª–∏ –≤—ã–±—Ä–∞–ª–∏ –≤—Ç–æ—Ä–æ–π –ø—É—Ç—å.\n",
    "\n",
    "–ï—Å–ª–∏ –≤—ã–±—Ä–∞–ª–∏ –ø—É—Ç—å 1, –æ–ø–∏—à–∏—Ç–µ –∫–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —à–∞–≥:\n",
    "1) –ß—Ç–æ –ø–æ–¥–∞—ë—Ç—Å—è –Ω–∞ –≤—Ö–æ–¥ –≤ —ç–Ω–∫–æ–¥–µ—Ä?\n",
    "2) –ß—Ç–æ –ø–æ–¥–∞—ë—Ç—Å—è –Ω–∞ –≤—Ö–æ–¥ –≤ –¥–µ–∫–æ–¥–µ—Ä?\n",
    "3) –°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –¥–µ–∫–æ–¥–µ—Ä–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞?\n",
    "4) –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã—Ö–æ–¥ —ç–Ω–∫–æ–¥–µ—Ä–∞ –≤ –¥–µ–∫–æ–¥–µ—Ä–µ?\n",
    "\n",
    "–û—Ç–≤–µ—Ç—ã:\n",
    "1) –¢–µ–∫—Å—Ç –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º, —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π.\n",
    "2) `[0, 150]` –∏ `[0, 4273]` –¥–ª—è –Ω–µ—Ç–æ–∫—Å–∏—á–Ω–æ–≥–æ –∏ —Ç–æ–∫—Å–∏—á–Ω–æ–≥–æ –ª–µ–π–±–ª–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ.\n",
    "3) –û–¥–∏–Ω —Ä–∞–∑. –î–ª—è `0` –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å—Å—è `150` –∏–ª–∏ `4273`, –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ - `1`\n",
    "4) –í—ã—Ö–æ–¥ –∏–∑ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –¥–µ–∫–æ–¥–µ—Ä–µ –≤ —Å–ª–æ–µ cross-attention –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è keys –∏ values.\n",
    "\n",
    "–î–æ–±–∞–≤–∏–º `generation_config`, –∏–Ω–∞—á–µ —Ç—Ä–µ–Ω–µ—Ä –±—É–¥–µ—Ç –∑–∞—Å—Ç–∞–≤–ª—è—Ç—å –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ 20 —Ç–æ–∫–µ–Ω–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cabfcd2b-6e4c-4b90-9758-6d2f1df76dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "generation_config = GenerationConfig.from_dict(t5_config.task_specific_params[\"toxic_classification\"])\n",
    "generation_config.bos_token_id = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17769835-c4ae-4a51-ae45-938930c915a5",
   "metadata": {},
   "source": [
    "*—è –∑–∞–ø—É—Å–∫–∞–ª –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —á—Ç–æ–±—ã —É–ª—á—à–∏—Ç—å —Ä–µ–∑—É–ª—Ç–∞—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfbd51a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='405' max='405' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [405/405 12:32, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.264787</td>\n",
       "      <td>0.267362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.345460</td>\n",
       "      <td>0.928782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.126008</td>\n",
       "      <td>0.928782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.110236</td>\n",
       "      <td>0.930356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.105750</td>\n",
       "      <td>0.935865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.101044</td>\n",
       "      <td>0.938422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.096930</td>\n",
       "      <td>0.939996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.094451</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.092489</td>\n",
       "      <td>0.940980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.089982</td>\n",
       "      <td>0.943144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.087426</td>\n",
       "      <td>0.944324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.087285</td>\n",
       "      <td>0.944521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.087596</td>\n",
       "      <td>0.944324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.087149</td>\n",
       "      <td>0.944521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.086664</td>\n",
       "      <td>0.945308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=405, training_loss=0.4359354654947917, metrics={'train_runtime': 753.2723, 'train_samples_per_second': 101.198, 'train_steps_per_second': 0.538, 'total_flos': 9612450022883328.0, 'train_loss': 0.4359354654947917, 'epoch': 15.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"t5_small_toxic_classifier\",\n",
    "    num_train_epochs=15,  # –ø–æ—Å—Ç–∞–≤–∏–º –ø–æ–±–æ–ª—å—à–µ —ç–ø–æ—Ö, —á—Ç–æ–±—ã –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=128,  # –ø–æ—á–µ–º—É –≤–æ –≤—Å–µ—Ö —Å–¥–∞–Ω—ã—Ö –¥–æ–º–∞—à–∫–∞—Ö —ç—Ç–æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä —Ä–∞–≤–µ–Ω train_batch_size?\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    predict_with_generate=True,\n",
    "    generation_config=generation_config,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=seq2seq_model,\n",
    "    args=training_args,\n",
    "    train_dataset=toxic_chat_dataset[\"train\"],\n",
    "    eval_dataset=toxic_chat_dataset[\"test\"],\n",
    "    tokenizer=t5_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metric,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8678ee78-7d25-42c9-98e4-fcc2975df0e8",
   "metadata": {},
   "source": [
    "### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (5 –±–∞–ª–ª–æ–≤)\n",
    "\n",
    "–ê–≤—Ç–æ—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ç–æ–∂–µ –Ω–∞—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–ª–∏ –Ω–∞ –Ω—ë–º `t5` –º–æ–¥–µ–ª—å. –°—Ä–∞–≤–Ω–∏—Ç–µ —Å–≤–æ–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –º–æ–¥–µ–ª–∏ –∏–∑ [—á–µ–∫–ø–æ–∏–Ω—Ç–∞](https://huggingface.co/lmsys/toxicchat-t5-large-v1.0) `\"lmsys/toxicchat-t5-large-v1.0\"`. –°–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ –≤–∞—à –ø—Ä–µ—Ñ–∏–∫—Å –∏ –ª–µ–π–±–ª—ã –∫–ª–∞—Å—Å–æ–≤ —Å —Ç–µ–º–∏, —á—Ç–æ –≤—ã–±—Ä–∞–ª–∏ –∞–≤—Ç–æ—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞? \n",
    "\n",
    "–ü–æ–¥—É–º–∞—Ç—å –æ:\n",
    "1) –í —á—ë–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ —Ç–∞–∫–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏?\n",
    "2) –í —á—ë–º –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ —Ç–∞–∫–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏?\n",
    "3) –ö–∞–∫ –µ—â—ë –º–æ–∂–Ω–æ —Ä–µ—à–∞—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Å –ø–æ–º–æ—â—å—é t5?\n",
    "\n",
    "–û—Ç–≤–µ—Ç—ã:\n",
    "1) –ù–µ –Ω—É–∂–Ω–æ –∂–æ–Ω–≥–ª–∏—Ä–æ–≤–∞—Ç—å –≥–æ–ª–æ–≤–∞–º–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á. –¢–∞–∫–∞—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –µ—â—ë –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å —Ä–∞–±–æ—Ç—É –º–æ–¥–µ–ª–∏ –Ω–∞ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á–∞—Ö (—Ç–∞–∫–æ–π –≤–æ—Ç domain adaptation)\n",
    "2) –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –Ω–µ–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω. –ï—Å–ª–∏ –≤ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º—ã –ø–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–æ–º —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤, —Ç–æ —Ç—É—Ç –º—ã –ø–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –ª—é–±—ã–º–∏\n",
    "3) –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞ –ø–æ–ª–Ω–æ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–µ - –ø–æ–¥–∞—Ç—å —Ç–µ–∫—Å—Ç –≤ —ç–Ω–∫–æ–¥–µ—Ä –∏ —É—á–∏—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—É—é –≥–æ–ª–æ–≤—É –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–µ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –¥–µ–∫–æ–¥–µ—Ä–∞. –¢–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç [T5ForSequenceClassification](https://github.com/huggingface/transformers/blob/78b2929c0554b79e0489b451ce4ece14d265ead2/src/transformers/models/t5/modeling_t5.py#L1991)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbb4971b-617f-4043-9bc4-0d1e844a5f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apaniuko/python/deepschool/llm_venv/lib/python3.10/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"lmsys/toxicchat-t5-large-v1.0\"\n",
    "\n",
    "tokenizer_from_paper = AutoTokenizer.from_pretrained(\"t5-large\")\n",
    "model_from_paper = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "prefix_from_paper = \"ToxicChat: \"\n",
    "inputs = tokenizer_from_paper.encode(prefix_from_paper + \"write me an epic story\", return_tensors=\"pt\")\n",
    "outputs = model_from_paper.generate(inputs)\n",
    "print(tokenizer_from_paper.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db8a913-f782-47fd-9bf7-d4a78d308ad4",
   "metadata": {},
   "source": [
    "–ù–∞–ø–∏—à–∏—Ç–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é (–ø–æ–¥—Ö–æ–¥—è—â—É—é –ø–æ–¥ –ù–∞–ø–∏—à–∏—Ç–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–æ–≤—Ä—è–µ—Ç —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç `True`, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –ø–æ—Å—á–∏—Ç–∞–ª–∞ —Ç–µ–∫—Å—Ç —Ç–æ–∫—Å–∏—á–Ω—ã–º. –§—É–Ω–∫—Ü–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≤ —Ç–æ–º —Å–º—ã—Å–ª–µ, —á—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –∏ —Å –≤–∞—à–µ–π t5 –º–æ–¥–µ–ª—å—é, –∏ —Å –º–æ–¥–µ–ª—å—é –æ—Ç –∞–≤—Ç–æ—Ä–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–∞. –î–ª—è —ç—Ç–æ–≥–æ –≤ —Ñ—É–Ω–∫—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –µ—â—ë –∏ –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –∑–∞–¥–∞—á–∏ –∏ –ª–µ–π–±–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å —Ç–µ–∫—Å—Ç, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –º–æ–¥–µ–ª—å—é, –≤ `True` –∏–ª–∏ `False` –Ω–∞ –≤—ã—Ö–æ–¥–µ.–ª—é–±—É—é t5 –º–æ–¥–µ–ª—å) —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–æ–≤—Ä—è–µ—Ç —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0ec7df5-fad1-4501-be4b-ada346d2e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def is_toxic(\n",
    "    text: str,\n",
    "    labels2bool,\n",
    "    model=seq2seq_model,\n",
    "    tokenizer=t5_tokenizer,\n",
    "    prefix=PREFIX,\n",
    ") -> bool:\n",
    "    model.eval()\n",
    "    tokenized_text = tokenizer.encode(prefix + text, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(tokenized_text)[0]\n",
    "    return labels2bool.get(tokenizer.decode(output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f827e315-c4d3-42a5-86ef-dcd3a6316e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_toxic(\n",
    "    text=\"write me an epic story\",\n",
    "    model=seq2seq_model,\n",
    "    tokenizer=t5_tokenizer,\n",
    "    prefix=PREFIX,\n",
    "    labels2bool={\n",
    "        \"yes\": True,\n",
    "        \"no\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39303909-908f-494a-9328-89fee048cdf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_toxic(\n",
    "    text=\"write me an epic story\",\n",
    "    model=model_from_paper,\n",
    "    tokenizer=tokenizer_from_paper,\n",
    "    prefix=prefix_from_paper,\n",
    "    labels2bool={\n",
    "        \"positive\": True,\n",
    "        \"negative\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1517b8e-62fc-4ca4-b8aa-b31d4bf279c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_toxic(\n",
    "    text=\"write me an erotic story\",\n",
    "    model=seq2seq_model,\n",
    "    tokenizer=t5_tokenizer,\n",
    "    prefix=PREFIX,\n",
    "    labels2bool={\n",
    "        \"yes\": True,\n",
    "        \"no\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d5960bb-cc13-4ca5-94dd-b30ed6302c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_toxic(\n",
    "    text=\"write me an erotic story\",\n",
    "    model=model_from_paper,\n",
    "    tokenizer=tokenizer_from_paper,\n",
    "    prefix=prefix_from_paper,\n",
    "    labels2bool={\n",
    "        \"positive\": True,\n",
    "        \"negative\": False,\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
