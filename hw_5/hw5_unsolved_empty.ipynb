{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11861b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# можете сменить на mps на макбуке, но лично у меня он криво работает\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a6c6c-64cc-4a1c-8b1e-3ccee36396d3",
   "metadata": {},
   "source": [
    "# Знакомство с Transformers - 35 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df5693",
   "metadata": {},
   "source": [
    "## Создание модели и предсказание следующего токена - 5 баллов\n",
    "Нужно создать модель через `AutoModelForCausalLM`, создать токенайзер через `AutoTokenizer` и олучить следующий токен через жадную генерацию!\n",
    "\n",
    "**Внимание** на каких-то из функций далее у вас может кончаться видеопамять из-за хранения активаций. Чтобы этого не происходило рекомендуется все вычисления оборачивать в контекстный менеджер `with torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec7e08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai-community/gpt2\"\n",
    "model = AutoModelForCausalLM # Ваш код здесь\n",
    "tokenizer = AutoTokenizer # ваш код здесь\n",
    "\n",
    "\n",
    "text = \"This is a sample text\"\n",
    "\n",
    "# Нужно преобразовать text с помощью tokenizer() и подать это в model.forward() (он же просто model())\n",
    "# после этого мы получим logits [batch_size = 1, seq_len, d_model]\n",
    "# По этому тензору нужно предсказать следующее слово!\n",
    "\n",
    "inputs = tokenizer(text, ...)\n",
    "\n",
    "outputs = model(...)\n",
    "logits = ...\n",
    "next_token_idx: int = ...\n",
    "\n",
    "\n",
    "next_token = tokenizer.decode([next_token_idx])\n",
    "\n",
    "assert next_token.strip() == \"file\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6809813",
   "metadata": {},
   "source": [
    "## Используем Generate - 5 баллов\n",
    "\n",
    "Мы с вами помним про различные виды сэмплинга - top_k, top_p, temperature,frequency penalty.\n",
    "Отличная новость заключается в том, что нам не нужно все это писать самим! Оно уже включено в [GenerationMixin](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#generation), от которого наследуются модели для генерации текста.\n",
    "\n",
    "Для генерации есть функция [generate](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n",
    "\n",
    "Ваша задача написать для модели выше генерацию по тексту с:\n",
    "* Температурой - 0.9\n",
    "* Top-K - 20\n",
    "* Repetition Penalty (Frequency Penalty) - 1.2\n",
    "* максимальное число новых токенов - 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b62dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is still a sample text, but\"\n",
    "inputs = tokenizer(text, ...)\n",
    "\n",
    "results = []\n",
    "for i in range(10):\n",
    "    gens = model.generate(\n",
    "        ...\n",
    "    )\n",
    "    genertaion: str = ... # сгенерированный текст\n",
    "    results.append(generation)\n",
    "\n",
    "assert len(set(results)) > 1, \"Все генерации получились одинаковыми, проверьте опции генерации и флаг do_sample!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90512b-9420-45b3-9f4c-22fb5fa1bfc7",
   "metadata": {},
   "source": [
    "## Generate Batched - 5\n",
    "Теперь давайте жадно сгенерируем текст, но забатчуем несколько сэмплов. До этого мы всегда генерировали по батчу размера 1, поэтому у нас не было паддингов!\n",
    "\n",
    "Когда появляется несколько текстов разной длины, то появляются и паддинги.\n",
    "\n",
    "Представим себе ситуцию, что у нас батч из двух элементов длины 2 и 5 (токен -1 будет выступать в качестве паддинга **только для удобства визуализации**).\n",
    "\n",
    "Тогда \n",
    "\n",
    "```python\n",
    "input_ids = [\n",
    "    [3, 2, -1, -1, -1]\n",
    "    [5, 6,  7,  1,  2]\n",
    "]\n",
    "attention_mask = [\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 1]\n",
    "]\n",
    "```\n",
    "\n",
    "Представим, что мы сгенерировали еще один токен, тогда\n",
    "\n",
    "```python\n",
    "input_ids = [\n",
    "    [3, 2, -1, -1, -1, 7]\n",
    "    [5, 6,  7,  1,  2, 8]\n",
    "]\n",
    "attention_mask = [\n",
    "    [1, 1, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 1, 1, 1]\n",
    "]\n",
    "```\n",
    "\n",
    "Получается, что у нас паддинги в маске возникают посередине. Мы не будем заниматься реализацией своего алгоритма генерации здесь, но отметим, что добавление паддинга слева значительно упрощает этот процесс.\n",
    "Тогда исходная последовательность будет:\n",
    "\n",
    "```python\n",
    "input_ids = [\n",
    "    [-1, -1, -1, 3, 2]\n",
    "    [ 5,  6,  7, 1, 2]\n",
    "]\n",
    "attention_mask = [\n",
    "    [0, 0, 0, 1, 1],\n",
    "    [1, 1, 1, 1, 1]\n",
    "]\n",
    "```\n",
    "\n",
    "и после генерации следующего токена\n",
    "\n",
    "```python\n",
    "input_ids = [\n",
    "    [-1, -1, -1, 3, 2, 7]\n",
    "    [ 5,  6,  7, 1, 2, 8]\n",
    "]\n",
    "attention_mask = [\n",
    "    [0, 0, 0, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1]\n",
    "]\n",
    "```\n",
    "\n",
    "В качестве задания давайте соберем батч с левым паддингом и проверим, что жадная генерация (10 токенов) совпадает с генерацией на текстах по отдельности!\n",
    "\n",
    "Для этого нам придется использовать параметр padding_side в конструкторе токенизатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db4cd76-b37b-4fd4-9cf8-8f76e04ae7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer # ваш код здесь\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd38bdc-3e5e-400d-8815-e9c08a757c03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts = [\"This is a sample text\", \"I'm really tired and this is just about\"]\n",
    "\n",
    "# Внимание! В данном задании нужна жадная генерация!\n",
    "\n",
    "# Соберите оба текста в один батч и положите результаты генерации в \n",
    "# batched_generations\n",
    "batched_generations: List[str] = []\n",
    "\n",
    "....\n",
    "\n",
    "# Пройдитесь по каждому сэмплу по отдельности и положите результаты генерации \n",
    "# в single_generations\n",
    "single_generations: List[str] = []\n",
    "\n",
    "...\n",
    "\n",
    "assert len(batched_generations) == 2 and len(single_generations) == 2\n",
    "for s, b in zip(batched_generations, single_generations):\n",
    "    assert s == b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9280196b-2526-4596-a534-07d5f368c8f2",
   "metadata": {},
   "source": [
    "# KV Cache - 10 баллов\n",
    "При генерации есть опция `use_cache` - это использование KV cache для генерации. Посмотреть визуализации про kv cache можно [тут](https://medium.com/@joaolages/kv-caching-explained-276520203249), или [тут](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/#key-value_caching), или [тут](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/transformers-neuronx/generative-llm-inference-with-neuron.html#kv-caching) или в лекции 4.\n",
    "\n",
    "В рамках этой техники в при генерации в декодере считается только аттеншн последнего токена по всем векторам предыдущих токенов, которые посчитали на предыдущих этапах, а для \"старых\" (левых) токенов аттеншн не пересчитывается, т.к. \"новые\" (правые) токены на них не влияют.\n",
    "\n",
    "Мы сохраняем K,V проекции предыдущих токенов (не пересчитываем $X W_K$ и $X W_V$) и считаем аттеншен только последнего токена по всем предыдущим. Аттеншены для предыдущих токенов мы не считаем, т.к. они для генерации не нужны, их выходы уже сохранениы в KV cache.\n",
    "\n",
    "В рамках данного задания нужно:\n",
    "1. Посчитать скорость генерации 100 токенов с и без kv cache, сказать, какая техника и во сколько раз быстрее.\n",
    "2. Подсчитать скорость генерации 1 токена с и без kv cache, сказать, какая техника быстрее и почему.\n",
    "\n",
    "Чтобы корректно сравнивать время генерации нужно использовать жадный сэмплинг!\n",
    "\n",
    "**Ответы на оба вопроса нужно оставить письменно прямо здесь**.\n",
    "\n",
    "<<Место для ответа>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aaccda-6934-4cfc-a803-fc482409c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "text = \"\"\"\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum lorem justo, semper dignissim ipsum vitae, sollicitudin aliquet eros. Duis id ultricies erat. Vivamus commodo auctor massa ut mollis. Maecenas lacinia tempus orci, imperdiet ullamcorper felis accumsan et. Etiam mattis neque diam, at egestas nunc eleifend id. Fusce tristique orci nec sollicitudin elementum. Nullam dui est, feugiat ac pellentesque at, posuere non massa.\n",
    "\n",
    "Suspendisse accumsan ullamcorper dolor sed dictum. Mauris quis varius felis, quis gravida odio. Vestibulum diam arcu, aliquet convallis congue non, rutrum non turpis. Fusce vel orci ac diam suscipit lacinia. Curabitur maximus orci a dui gravida, accumsan convallis libero ornare. Phasellus dapibus, sapien pulvinar lacinia dictum, massa lacus scelerisque tellus, eu porta dolor eros vitae ex. Maecenas maximus, urna id pharetra dictum, dolor lorem sollicitudin ipsum, sit amet vestibulum orci felis quis leo. Pellentesque vel ligula ut urna eleifend condimentum nec et sem. Integer ligula nunc, rutrum ultricies urna et, congue suscipit lectus.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Подсчитаейте время генерации 100 новых токенов с помощью жадного сэмплирования при включенном KV Cache, выведите среднее время работы\n",
    "...\n",
    "\n",
    "# Подсчитаейте время генерации 100 новых токенов с помощью жадного сэмплирования при выключенном KV Cache, выведите среднее время работы\n",
    "...\n",
    "\n",
    "# Не забудьте ответить на вопросы вы описании задания!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1a3f3-741c-4b9e-9ae5-73446d3e7b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum lorem justo, semper dignissim ipsum vitae, sollicitudin aliquet eros. Duis id ultricies erat. Vivamus commodo auctor massa ut mollis. Maecenas lacinia tempus orci, imperdiet ullamcorper felis accumsan et. Etiam mattis neque diam, at egestas nunc eleifend id. Fusce tristique orci nec sollicitudin elementum. Nullam dui est, feugiat ac pellentesque at, posuere non massa.\n",
    "\n",
    "Suspendisse accumsan ullamcorper dolor sed dictum. Mauris quis varius felis, quis gravida odio. Vestibulum diam arcu, aliquet convallis congue non, rutrum non turpis. Fusce vel orci ac diam suscipit lacinia. Curabitur maximus orci a dui gravida, accumsan convallis libero ornare. Phasellus dapibus, sapien pulvinar lacinia dictum, massa lacus scelerisque tellus, eu porta dolor eros vitae ex. Maecenas maximus, urna id pharetra dictum, dolor lorem sollicitudin ipsum, sit amet vestibulum orci felis quis leo. Pellentesque vel ligula ut urna eleifend condimentum nec et sem. Integer ligula nunc, rutrum ultricies urna et, congue suscipit lectus.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Подсчитаейте время генерации 1 нового токена с помощью жадного сэмплирования при включенном KV Cache, выведите среднее время работы\n",
    "...\n",
    "\n",
    "# Подсчитаейте время генерации 1 нового токена с помощью жадного сэмплирования при выключенном KV Cache, выведите среднее время работы\n",
    "...\n",
    "\n",
    "# Не забудьте ответить на вопросы вы описании задания!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5da008c-3653-40d5-89ba-cd831352fd3d",
   "metadata": {},
   "source": [
    "# Скоринг, Perplixity - 6 баллов\n",
    "\n",
    "Можно не только генерировать текст. Вспомним, что выдает после lm_head - вектор `[batch_size, seq_len, vocab_size]`, где для каждый вектор `[vocab_size]` это распределение вероятностей по следующему токену!\n",
    "\n",
    "Опустим размерность batch_size=1 для удобства, seq_len = 4. Пусть у нас есть текст `bos мама мыла раму` (`bos` спецсимвол для начала текста)\n",
    "\n",
    "Тогда вероятность этого текста расписывается через произведение условных вероятностей:\n",
    "\n",
    "```\n",
    "P(bos мама мыла раму) = P(мама | bos) * P(мыла | bos мама) * P(раму| bos мама мыла)\n",
    "```\n",
    "\n",
    "Т.е. это вероятность слова при условии его левого контекста.\n",
    "Зачастую ее обозначают как $P(x_i|x_{<i})$ где $x_i$ - i-е слово, $x_{<i}$ - контекст $[x_1, x_2, x_3, ... x_{i-1}]$\n",
    "Эти вероятности можно взять из выходного вектора!\n",
    "\n",
    "Давайте попробуем подсчитать вероятность и perplexity текстов!\n",
    "perplexity как и вероятность мера того насколько модель \"уверена\" в тексте, т.е. насколько по оценки ее параметрами данный текст вероятен.\n",
    "\n",
    "$$Perplexity(X) = exp(-\\frac {1} {N} \\sum_{i}^{N} log P(x_i | x_{<i}))$$\n",
    "\n",
    "В этом задании нужно:\n",
    "1. Посчитать вероятность **text**\n",
    "2. Посчитать перплексию **text**\n",
    "\n",
    "Еще одна важная деталь:\n",
    "работать с вероятностями плохо. Т.к. вероятность представляет собой число от 0 до 1, то при перемножении десятков или даже сотен таких числе теряется точность!\n",
    "Для этого от произведения вероятностей берут логарифм и получают logprobs - логарифмы вероятностей. Их можно складывать, по свойству логарифма логарифм произведения равен произведению логарифма.\n",
    "\n",
    "$$ p = p_1 * p_2 * p_3 $$\n",
    "$$log(p) = log (p_1) + log (p_2) + log (p_3)$$\n",
    "$$exp(log (p)) = p = exp(log (p_1) + log (p_2) + log (p_3)) = exp (log (p_1 * p_2 * p_3)) = p_1 * p_2 * p_3$$\n",
    "\n",
    "В pytorch для этого есть `torch.log_softmax`, который считается численно стабильно!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7ba39-a451-43a2-ac55-629c99259abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Beginning of sentence (BOS) token = `{tokenizer.bos_token}`\")\n",
    "print(f\"End of sentence (EOS) token  = `{tokenizer.eos_token}`\")\n",
    "text = \"<|endoftext|>I'm so very tired of this<|endoftext|>\"\n",
    "\n",
    "inputs = tokenizer(text, ...)\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(...).logits\n",
    "    ...\n",
    "    # ваш код здесь!\n",
    "    # 1. Нужно обрезать logits по длине, т.к. для предсказаний по последнему токену нечего считать\n",
    "    # 2. Превращаем logits в log_probs\n",
    "    # 3. Берем вероятности следующих токенов, т.к. по вектору i-й позиции мы предсказываем токен на позиции (i + 1)\n",
    "    # для этого нам поможет torch.gather\n",
    "    # 4. Считаем вероятности и perplexity!\n",
    "\n",
    "\n",
    "# должно получиться что-то около 2.1783e-14 для вероятности и около 51 для ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f244eac-7cb1-4689-8adc-46662891e657",
   "metadata": {},
   "source": [
    "# Вопросы - 4 балла\n",
    "\n",
    "**Ответьте на вопрсоы текстом прямо здесь!**\n",
    "\n",
    "\n",
    "1. Какое значение P(X) вероятности текста самое \"лучшее\" в том смысле, что модель максимально уверена в этом тексте и скорее всего его сгенерирует.\n",
    "2. Какое значение перплексии текста самое \"лучшее\" в том смысле, что модель максимально уверена в этом тексте и скорее всего его сгенерирует.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd5038-620b-48bb-bbc1-db3729141d78",
   "metadata": {},
   "source": [
    "# Chat-Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c7530-a7ce-4d23-abaf-2b0cec87301e",
   "metadata": {},
   "source": [
    "# Формат - 5 баллов\n",
    "Как мы обсуждали на лекции, все chat-модели принимают входы в своем особом формате.\n",
    "Он может быть описан текстом, а может быть заложен в шаблон, который доступен через `tokenizer.apply_chat_template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5fe593-63a8-406d-9678-6d805c180670",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Meta-Llama-3-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.half).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8dad11-6811-49ab-ad3d-8ac7de103828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(d):\n",
    "    for k, v in d.items():\n",
    "        d[k] = v.to(device)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503f15fd-576a-4e8e-917a-74df01a944f4",
   "metadata": {},
   "source": [
    "Давайте посмотрим, как chat модель отработает на обычном тексте. Используйте для генерации сэмплинг и kv cache, выведите 5 результатов генерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7134f0bb-1ee4-4508-a26d-5326ea96562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hello how are you\"\n",
    "inputs = tokenizer(text, ...)\n",
    "\n",
    "for i in range(5):\n",
    "    # model.generate...\n",
    "    generated_text = ...\n",
    "    print(generated_text)\n",
    "    print(\"====\" * 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd50470-64b9-4a21-8748-0e9c5ea439fc",
   "metadata": {},
   "source": [
    "Видим, что текст зачастую выходит мусорный. Это потому что формат входных данных сильно отличается от того, что модель видела на обучении.\n",
    "Как мы уже обсуждали, у всех chat-моделей свой формат. Где-то он описан просто словами, где-то он заложен в токенайзер. Мы рассмотрим как раз такой случай - за нас есть удобно написанная функция `apply_chat_template`. Давайте используем ее, чтобы получить префикс для генерации модели.\n",
    "\n",
    "Не забудьте про опцию add_generation_prefix - она добавляет часть формата, после которой ожидается ответ модели!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a3701-c80f-4b90-90bd-fa010e32ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"hello\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"I'm good. How can I help you today\"},\n",
    "    {\"role\": \"user\", \"content\": \"I love you\"},\n",
    "]\n",
    "\n",
    "prefix = tokenizer.apply_chat_template(...)\n",
    "\n",
    "reference = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "I'm good. How can I help you today<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "I love you<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "assert prefix.strip() == reference.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048b8882-bec0-4bf4-b6fb-30e727d095c6",
   "metadata": {},
   "source": [
    "Давайте посмотрим, что нам ответит модель!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4284b18d-4f9b-4e7d-b3ea-bb365e90093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prefix, ...)\n",
    "model.generate...\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72482f3-c296-46f3-851c-57b4f91a717b",
   "metadata": {},
   "source": [
    "## Benchmark - 15 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f422a9-c2ee-4c17-8aee-1830f1d143e6",
   "metadata": {},
   "source": [
    "Перед нами датасет MMLU - датасет вопросов и ответов в стиле multiple choice.\n",
    "* question - вопрос\n",
    "* choices - варианты ответа\n",
    "* answer - номер правильного ответа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d1721-6623-4ca6-816c-d4f90203ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "mmlu = load_dataset(\"cais/mmlu\", \"global_facts\", split=\"test\")\n",
    "mmlu[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca61a91-5784-44f3-af9b-72250f8d58a4",
   "metadata": {},
   "source": [
    "Наша задача здесь решить задачу многоклассовой классификации.\n",
    "Для этого нужно посчитать \n",
    "$$P(choices_i | question)$$\n",
    "т.е. для посчитать вероятность каждого варианта ответа для вопроса. Мы это уже делали кодом выше!\n",
    "\n",
    "После этого давайте брать самый вероятный ответ и считать, что модель его выбрала.\n",
    "После этого давайте посчитаем accurracy, т.е. долю правильных ответов.\n",
    "Вместо вероятностей для подсчета лучше использовать logprobs.\n",
    "\n",
    "Итого, что нужно сделать:\n",
    "1. Пройтись по датасету, для каждого question и каждого из соответствующих choices получить самый вероятный ответ.\n",
    "2. Посчитать итоговый accuracy\n",
    "\n",
    "* За базовую реализацию с двойным циклом по questions и choices дается 5 баллов.\n",
    "* еще 3 балла дается, если собрать question и все его choices в батч размера 4 и считать вероятности параллельно для всего батча сразу.\n",
    "* еще 3 балла дается, если собрать несколько questions и их choces в батч размера 4 * num_questions_in_batch и считать вероятности параллельно для всего батча сразу.\n",
    "\n",
    "Что важно помнить:\n",
    "1. Данные нужно подавать в правильном формате для llama3!\n",
    "2. Если делаете варианты с батчеванием помните: длины choices могут быть разными! Нужно не считать вероятности по паддингам. В этом нам помогут attention_masks из выходов `tokenizer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4126e2-c463-404d-a3b5-5361f744242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваш код здесь!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9c63f-5d9e-4204-b1a9-d93ca19c1888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76c440af-dfc0-460d-a113-3fab3fefa361",
   "metadata": {},
   "source": [
    "**Ответьте на следующие вопросы (2 балла за вопрос)**:\n",
    "1. Как влияет длина ответа на вероятность ответа при скоринге? Если есть какие-либо проблемы, как бы вы с этим боролись.\n",
    "2. Если к началу каждого ответа добавилить метки A) B) C) D) станет ли модель отвечать лучше или хуже?\n",
    "Стоит ли по-вашему добавлять эти метки?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835f1a9-05fb-440f-93b4-01c53419eb96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
