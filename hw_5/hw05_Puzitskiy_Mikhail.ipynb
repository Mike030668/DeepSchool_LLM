{"cells":[{"cell_type":"code","execution_count":null,"id":"11861b12","metadata":{"id":"11861b12"},"outputs":[],"source":["from typing import List\n","\n","import torch\n","import torch.nn as nn\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","\n","# можете сменить на mps на макбуке, но лично у меня он криво работает\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"]},{"cell_type":"markdown","id":"ca7a6c6c-64cc-4a1c-8b1e-3ccee36396d3","metadata":{"id":"ca7a6c6c-64cc-4a1c-8b1e-3ccee36396d3"},"source":["# Знакомство с Transformers - 35 баллов"]},{"cell_type":"markdown","id":"a3df5693","metadata":{"id":"a3df5693"},"source":["## Создание модели и предсказание следующего токена - 5 баллов\n","Нужно создать модель через `AutoModelForCausalLM`, создать токенайзер через `AutoTokenizer` и олучить следующий токен через жадную генерацию!\n","\n","**Внимание** на каких-то из функций далее у вас может кончаться видеопамять из-за хранения активаций. Чтобы этого не происходило рекомендуется все вычисления оборачивать в контекстный менеджер `with torch.no_grad()`"]},{"cell_type":"code","execution_count":null,"id":"6ec7e08b","metadata":{"id":"6ec7e08b"},"outputs":[],"source":["model_name = \"openai-community/gpt2\"\n","# Загружаем модель для причинного языкового моделирования\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","# Загружаем соответствующий токенайзер\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","\n","text = \"This is a sample text\"\n","\n","# Нужно преобразовать text с помощью tokenizer() и подать это в model.forward() (он же просто model())\n","# после этого мы получим logits [batch_size = 1, seq_len, d_model]\n","# По этому тензору нужно предсказать следующее слово!\n","\n","# Преобразуем текст в токены и создаем тензор ввода\n","inputs = tokenizer(text, return_tensors=\"pt\")\n","\n","# Отключаем вычисление градиентов для экономии видеопамяти\n","with torch.no_grad():\n","    # Пропускаем вход через модель и получаем логиты\n","    outputs = model(**inputs)\n","    logits = outputs.logits\n","\n","# Логиты имеют размер [batch_size, seq_len, vocab_size]\n","# Мы берем логиты последнего токена в последовательности\n","last_token_logits = logits[:, -1, :]\n","next_token_idx = last_token_logits.argmax(dim=-1).item()\n","# Декодируем индекс токена обратно в строку\n","next_token = tokenizer.decode([next_token_idx])\n","\n","\n","next_token = tokenizer.decode([next_token_idx])\n","\n","assert next_token.strip() == \"file\"\n","\n"]},{"cell_type":"markdown","id":"e6809813","metadata":{"id":"e6809813"},"source":["## Используем Generate - 5 баллов\n","\n","Мы с вами помним про различные виды сэмплинга - top_k, top_p, temperature,frequency penalty.\n","Отличная новость заключается в том, что нам не нужно все это писать самим! Оно уже включено в [GenerationMixin](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#generation), от которого наследуются модели для генерации текста.\n","\n","Для генерации есть функция [generate](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n","\n","Ваша задача написать для модели выше генерацию по тексту с:\n","* Температурой - 0.9\n","* Top-K - 20\n","* Repetition Penalty (Frequency Penalty) - 1.2\n","* максимальное число новых токенов - 10\n"]},{"cell_type":"code","execution_count":null,"id":"a6b62dbf","metadata":{"id":"a6b62dbf"},"outputs":[],"source":["text = \"This is still a sample text, but\"\n","\n","# Преобразуем текст в токены и создаем тензор ввода\n","inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","\n","# Список для хранения результатов генерации\n","results = []\n","\n","# Отключаем вычисление градиентов для экономии видеопамяти\n","with torch.no_grad():\n","  for i in range(10):\n","      # Генерация текста с заданными параметрами сэмплинга\n","      gens = model.generate(\n","            inputs.input_ids,\n","            do_sample=True,               # Включаем сэмплинг для разнообразия\n","            temperature=0.9,              # Температура сэмплинга\n","            top_k=20,                     # Top-K сэмплинг\n","            repetition_penalty=1.2,       # Штраф за повторение\n","            max_new_tokens=10,            # Максимум новых токенов\n","            pad_token_id=tokenizer.eos_token_id  # Идентификатор токена заполнения\n","        )\n","              # Декодируем сгенерированные токены обратно в строку\n","      generation = tokenizer.decode(gens[0], skip_special_tokens=True) # сгенерированный текст\n","      results.append(generation)\n","\n","assert len(set(results)) > 1, \"Все генерации получились одинаковыми, проверьте опции генерации и флаг do_sample!\""]},{"cell_type":"markdown","id":"8b90512b-9420-45b3-9f4c-22fb5fa1bfc7","metadata":{"id":"8b90512b-9420-45b3-9f4c-22fb5fa1bfc7"},"source":["## Generate Batched - 5\n","Теперь давайте жадно сгенерируем текст, но забатчуем несколько сэмплов. До этого мы всегда генерировали по батчу размера 1, поэтому у нас не было паддингов!\n","\n","Когда появляется несколько текстов разной длины, то появляются и паддинги.\n","\n","Представим себе ситуцию, что у нас батч из двух элементов длины 2 и 5 (токен -1 будет выступать в качестве паддинга **только для удобства визуализации**).\n","\n","Тогда\n","\n","```python\n","input_ids = [\n","    [3, 2, -1, -1, -1]\n","    [5, 6,  7,  1,  2]\n","]\n","attention_mask = [\n","    [1, 1, 0, 0, 0],\n","    [1, 1, 1, 1, 1]\n","]\n","```\n","\n","Представим, что мы сгенерировали еще один токен, тогда\n","\n","```python\n","input_ids = [\n","    [3, 2, -1, -1, -1, 7]\n","    [5, 6,  7,  1,  2, 8]\n","]\n","attention_mask = [\n","    [1, 1, 0, 0, 0, 1],\n","    [1, 1, 1, 1, 1, 1]\n","]\n","```\n","\n","Получается, что у нас паддинги в маске возникают посередине. Мы не будем заниматься реализацией своего алгоритма генерации здесь, но отметим, что добавление паддинга слева значительно упрощает этот процесс.\n","Тогда исходная последовательность будет:\n","\n","```python\n","input_ids = [\n","    [-1, -1, -1, 3, 2]\n","    [ 5,  6,  7, 1, 2]\n","]\n","attention_mask = [\n","    [0, 0, 0, 1, 1],\n","    [1, 1, 1, 1, 1]\n","]\n","```\n","\n","и после генерации следующего токена\n","\n","```python\n","input_ids = [\n","    [-1, -1, -1, 3, 2, 7]\n","    [ 5,  6,  7, 1, 2, 8]\n","]\n","attention_mask = [\n","    [0, 0, 0, 1, 1, 1],\n","    [1, 1, 1, 1, 1, 1]\n","]\n","```\n","\n","В качестве задания давайте соберем батч с левым паддингом и проверим, что жадная генерация (10 токенов) совпадает с генерацией на текстах по отдельности!\n","\n","Для этого нам придется использовать параметр padding_side в конструкторе токенизатора."]},{"cell_type":"code","execution_count":null,"id":"5db4cd76-b37b-4fd4-9cf8-8f76e04ae7a1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5db4cd76-b37b-4fd4-9cf8-8f76e04ae7a1","executionInfo":{"status":"ok","timestamp":1727793925611,"user_tz":-180,"elapsed":664,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"89f2f1a3-9f9c-44ee-cf45-244eb6f0115f"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["# Загружаем токенайзер с левым паддингом\n","tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n","tokenizer.pad_token_id = tokenizer.eos_token_id  # Устанавливаем pad_token_id"]},{"cell_type":"code","source":["# Переносим модель на доступное устройство (GPU, если доступно)\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cjWZxm83jA8W","executionInfo":{"status":"ok","timestamp":1727793998643,"user_tz":-180,"elapsed":326,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"a36853c7-b820-42e8-dc0f-fc555efa1874"},"id":"cjWZxm83jA8W","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2SdpaAttention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","execution_count":null,"id":"1bd38bdc-3e5e-400d-8815-e9c08a757c03","metadata":{"scrolled":true,"id":"1bd38bdc-3e5e-400d-8815-e9c08a757c03"},"outputs":[],"source":["texts = [\"This is a sample text\", \"I'm really tired and this is just about\"]\n","\n","# Внимание! В данном задании нужна жадная генерация!\n","\n","# Соберите оба текста в один батч и положите результаты генерации в\n","# batched_generations\n","batched_generations: List[str] = []\n","\n","# Отключаем вычисление градиентов для экономии видеопамяти\n","with torch.no_grad():\n","    # ----------- Батч генерация ----------- #\n","    # Преобразуем тексты в токены с левым паддингом\n","    batch_inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(device)\n","\n","    # Выполняем жадную генерацию (без сэмплинга)\n","    batch_outputs = model.generate(\n","        input_ids=batch_inputs.input_ids,\n","        attention_mask=batch_inputs.attention_mask,\n","        do_sample=False,            # Жадная генерация\n","        max_new_tokens=10,          # Максимум новых токенов\n","        pad_token_id=tokenizer.eos_token_id  # Идентификатор токена заполнения\n","    )\n","\n","    # Декодируем сгенерированные токены обратно в строки\n","    batched_generations = tokenizer.batch_decode(batch_outputs, skip_special_tokens=True)\n","\n","\n","\n","    # Пройдитесь по каждому сэмплу по отдельности и положите результаты генерации\n","    # в single_generations\n","    single_generations: List[str] = []\n","    # ----------- Индивидуальная генерация ----------- #\n","    for text in texts:\n","        # Токенизируем отдельный текст с левым паддингом\n","        single_input = tokenizer(text, return_tensors=\"pt\", padding='longest').to(device)\n","\n","        # Выполняем жадную генерацию\n","        single_output = model.generate(\n","            input_ids=single_input.input_ids,\n","            attention_mask=single_input.attention_mask,\n","            do_sample=False,            # Жадная генерация\n","            max_new_tokens=10,          # Максимум новых токенов\n","            pad_token_id=tokenizer.eos_token_id  # Идентификатор токена заполнения\n","        )\n","\n","        # Декодируем сгенерированные токены обратно в строку\n","        generated_text = tokenizer.decode(single_output[0], skip_special_tokens=True)\n","        single_generations.append(generated_text)\n","\n","\n","\n","    assert len(batched_generations) == 2 and len(single_generations) == 2\n","    for s, b in zip(batched_generations, single_generations):\n","        assert s == b"]},{"cell_type":"markdown","id":"9280196b-2526-4596-a534-07d5f368c8f2","metadata":{"id":"9280196b-2526-4596-a534-07d5f368c8f2"},"source":["# KV Cache - 10 баллов\n","При генерации есть опция `use_cache` - это использование KV cache для генерации. Посмотреть визуализации про kv cache можно [тут](https://medium.com/@joaolages/kv-caching-explained-276520203249), или [тут](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/#key-value_caching), или [тут](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/transformers-neuronx/generative-llm-inference-with-neuron.html#kv-caching) или в лекции 4.\n","\n","В рамках этой техники в при генерации в декодере считается только аттеншн последнего токена по всем векторам предыдущих токенов, которые посчитали на предыдущих этапах, а для \"старых\" (левых) токенов аттеншн не пересчитывается, т.к. \"новые\" (правые) токены на них не влияют.\n","\n","Мы сохраняем K,V проекции предыдущих токенов (не пересчитываем $X W_K$ и $X W_V$) и считаем аттеншен только последнего токена по всем предыдущим. Аттеншены для предыдущих токенов мы не считаем, т.к. они для генерации не нужны, их выходы уже сохранениы в KV cache.\n","\n","В рамках данного задания нужно:\n","1. Посчитать скорость генерации 100 токенов с и без kv cache, сказать, какая техника и во сколько раз быстрее.\n","2. Подсчитать скорость генерации 1 токена с и без kv cache, сказать, какая техника быстрее и почему.\n","\n","Чтобы корректно сравнивать время генерации нужно использовать жадный сэмплинг!\n","\n","**Ответы на оба вопроса нужно оставить письменно прямо здесь**.\n","\n","<<Место для ответа>>\n","  - Среднее время генерации 100 токенов с KV Cache: 14.8756 секунд\n","  - Среднее время генерации 100 токенов без KV Cache: 241.5391 секунд\n","  - Отношениее в 16.0 раз"]},{"cell_type":"code","source":["# Функция для измерения времени генерации\n","def measure_generation_time(model, tokenizer, input_text, max_new_tokens, use_cache, num_runs=1):\n","    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n","    total_time = 0.0\n","\n","    with torch.no_grad():\n","        for _ in range(num_runs):\n","            start_time = time.perf_counter()\n","            outputs = model.generate(\n","                input_ids=inputs.input_ids,\n","                attention_mask=inputs.attention_mask,\n","                max_new_tokens=max_new_tokens,\n","                do_sample=False,       # Жадная генерация\n","                use_cache=use_cache,   # Использование KV Cache\n","                pad_token_id=tokenizer.pad_token_id\n","            )\n","            if device.type == 'cuda':\n","                torch.cuda.synchronize()  # Ожидание завершения всех операций на GPU\n","            end_time = time.perf_counter()\n","            total_time += (end_time - start_time)\n","\n","    average_time = total_time / num_runs\n","    return average_time\n","\n","\n","# Функция для измерения времени генерации 1 токена за раз\n","def measure_single_token_generation_time(model, tokenizer, input_text, num_tokens, use_cache):\n","    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n","    generated_ids = inputs.input_ids\n","    attention_mask = inputs.attention_mask\n","\n","    total_time = 0.0\n","\n","    with torch.no_grad():\n","        for _ in range(num_tokens):\n","            start_time = time.perf_counter()\n","            outputs = model.generate(\n","                input_ids=generated_ids,\n","                attention_mask=attention_mask,\n","                max_new_tokens=1,\n","                do_sample=False,       # Жадная генерация\n","                use_cache=use_cache,   # Использование KV Cache\n","                pad_token_id=tokenizer.pad_token_id\n","            )\n","            if device.type == 'cuda':\n","                torch.cuda.synchronize()  # Ожидание завершения всех операций на GPU\n","            end_time = time.perf_counter()\n","            total_time += (end_time - start_time)\n","\n","            # Обновляем input_ids и attention_mask для следующей итерации\n","            generated_ids = outputs\n","            attention_mask = torch.ones_like(generated_ids)\n","\n","    average_time = total_time / num_tokens\n","    return average_time"],"metadata":{"id":"jq3M_hm8l0Yv"},"id":"jq3M_hm8l0Yv","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"72aaccda-6934-4cfc-a803-fc482409c7eb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72aaccda-6934-4cfc-a803-fc482409c7eb","executionInfo":{"status":"ok","timestamp":1727795351132,"user_tz":-180,"elapsed":256770,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"6e40b7d7-63a8-48b9-a7f8-4439c40af946"},"outputs":[{"output_type":"stream","name":"stdout","text":["Среднее время генерации 100 токенов с KV Cache: 14.8756 секунд\n","Среднее время генерации 100 токенов без KV Cache: 241.5391 секунд\n"]}],"source":["import time\n","import numpy as np\n","\n","text = \"\"\"\n","Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum lorem justo, semper dignissim ipsum vitae, sollicitudin aliquet eros. Duis id ultricies erat. Vivamus commodo auctor massa ut mollis. Maecenas lacinia tempus orci, imperdiet ullamcorper felis accumsan et. Etiam mattis neque diam, at egestas nunc eleifend id. Fusce tristique orci nec sollicitudin elementum. Nullam dui est, feugiat ac pellentesque at, posuere non massa.\n","\n","Suspendisse accumsan ullamcorper dolor sed dictum. Mauris quis varius felis, quis gravida odio. Vestibulum diam arcu, aliquet convallis congue non, rutrum non turpis. Fusce vel orci ac diam suscipit lacinia. Curabitur maximus orci a dui gravida, accumsan convallis libero ornare. Phasellus dapibus, sapien pulvinar lacinia dictum, massa lacus scelerisque tellus, eu porta dolor eros vitae ex. Maecenas maximus, urna id pharetra dictum, dolor lorem sollicitudin ipsum, sit amet vestibulum orci felis quis leo. Pellentesque vel ligula ut urna eleifend condimentum nec et sem. Integer ligula nunc, rutrum ultricies urna et, congue suscipit lectus.\n","\"\"\".strip()\n","\n","\n","\n","# Количество токенов для генерации\n","num_tokens = 100\n","\n","# Подсчитаейте время генерации 100 новых токенов с помощью жадного сэмплирования при включенном KV Cache, выведите среднее время работы\n","time_with_cache = measure_generation_time(\n","    model=model,\n","    tokenizer=tokenizer,\n","    input_text=text,\n","    max_new_tokens=num_tokens,\n","    use_cache=True,\n","    num_runs=1  # к-во раз для более точного измерения\n",")\n","\n","print(f\"Среднее время генерации {num_tokens} токенов с KV Cache: {time_with_cache:.4f} секунд\")\n","\n","\n","# Подсчитаейте время генерации 100 новых токенов с помощью жадного сэмплирования при выключенном KV Cache, выведите среднее время работы\n","time_without_cache = measure_generation_time(\n","    model=model,\n","    tokenizer=tokenizer,\n","    input_text=text,\n","    max_new_tokens=num_tokens,\n","    use_cache=False,\n","   num_runs=1  # к-во раз для более точного измерения\n",")\n","\n","print(f\"Среднее время генерации {num_tokens} токенов без KV Cache: {time_without_cache:.4f} секунд\")\n","\n","# Не забудьте ответить на вопросы вы описании задания!\n"]},{"cell_type":"code","source":["print(f\"Отношениее в {time_without_cache//time_with_cache} раз\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WUhLis7NoYyU","executionInfo":{"status":"ok","timestamp":1727795489198,"user_tz":-180,"elapsed":343,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"d2023152-8cf0-4bf9-ba37-765f4aefabdf"},"id":"WUhLis7NoYyU","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Отношениее в 16.0 раз\n"]}]},{"cell_type":"code","execution_count":null,"id":"bdd1a3f3-741c-4b9e-9ae5-73446d3e7b6e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdd1a3f3-741c-4b9e-9ae5-73446d3e7b6e","executionInfo":{"status":"ok","timestamp":1727796462054,"user_tz":-180,"elapsed":491249,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"5cea808e-0960-4ca1-e33e-86f3f891cc1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Среднее время генерации 1 токена с KV Cache: 2.481780 секунд\n","Среднее время генерации 1 токена без KV Cache: 2.427287 секунд\n","Отношениее в 0.0 раз\n"]}],"source":["text = \"\"\"\n","Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum lorem justo, semper dignissim ipsum vitae, sollicitudin aliquet eros. Duis id ultricies erat. Vivamus commodo auctor massa ut mollis. Maecenas lacinia tempus orci, imperdiet ullamcorper felis accumsan et. Etiam mattis neque diam, at egestas nunc eleifend id. Fusce tristique orci nec sollicitudin elementum. Nullam dui est, feugiat ac pellentesque at, posuere non massa.\n","\n","Suspendisse accumsan ullamcorper dolor sed dictum. Mauris quis varius felis, quis gravida odio. Vestibulum diam arcu, aliquet convallis congue non, rutrum non turpis. Fusce vel orci ac diam suscipit lacinia. Curabitur maximus orci a dui gravida, accumsan convallis libero ornare. Phasellus dapibus, sapien pulvinar lacinia dictum, massa lacus scelerisque tellus, eu porta dolor eros vitae ex. Maecenas maximus, urna id pharetra dictum, dolor lorem sollicitudin ipsum, sit amet vestibulum orci felis quis leo. Pellentesque vel ligula ut urna eleifend condimentum nec et sem. Integer ligula nunc, rutrum ultricies urna et, congue suscipit lectus.\n","\"\"\".strip()\n","\n","# Подсчитаейте время генерации 1 нового токена с помощью жадного сэмплирования при включенном KV Cache, выведите среднее время работы\n","# Измерение времени генерации 1 токена с KV Cache (повторяем 100 раз)\n","time_single_with_cache = measure_single_token_generation_time(\n","    model=model,\n","    tokenizer=tokenizer,\n","    input_text=text,\n","    num_tokens=100,\n","    use_cache=True\n",")\n","\n","\n","print(f\"Среднее время генерации 1 токена с KV Cache: {time_single_with_cache:.6f} секунд\")\n","\n","\n","\n","# Подсчитаейте время генерации 1 нового токена с помощью жадного сэмплирования при выключенном KV Cache, выведите среднее время работы\n","time_single_without_cache = measure_single_token_generation_time(\n","    model=model,\n","    tokenizer=tokenizer,\n","    input_text=text,\n","    num_tokens=100,\n","    use_cache=False\n",")\n","\n","print(f\"Среднее время генерации 1 токена без KV Cache: {time_single_without_cache:.6f} секунд\")\n","\n","# Не забудьте ответить на вопросы вы описании задания!\n","print(f\"Отношениее в {time_single_without_cache//time_single_with_cache} раз\")"]},{"cell_type":"markdown","id":"f5da008c-3653-40d5-89ba-cd831352fd3d","metadata":{"id":"f5da008c-3653-40d5-89ba-cd831352fd3d"},"source":["# Скоринг, Perplixity - 6 баллов\n","\n","Можно не только генерировать текст. Вспомним, что выдает после lm_head - вектор `[batch_size, seq_len, vocab_size]`, где для каждый вектор `[vocab_size]` это распределение вероятностей по следующему токену!\n","\n","Опустим размерность batch_size=1 для удобства, seq_len = 4. Пусть у нас есть текст `bos мама мыла раму` (`bos` спецсимвол для начала текста)\n","\n","Тогда вероятность этого текста расписывается через произведение условных вероятностей:\n","\n","```\n","P(bos мама мыла раму) = P(мама | bos) * P(мыла | bos мама) * P(раму| bos мама мыла)\n","```\n","\n","Т.е. это вероятность слова при условии его левого контекста.\n","Зачастую ее обозначают как $P(x_i|x_{<i})$ где $x_i$ - i-е слово, $x_{<i}$ - контекст $[x_1, x_2, x_3, ... x_{i-1}]$\n","Эти вероятности можно взять из выходного вектора!\n","\n","Давайте попробуем подсчитать вероятность и perplexity текстов!\n","perplexity как и вероятность мера того насколько модель \"уверена\" в тексте, т.е. насколько по оценки ее параметрами данный текст вероятен.\n","\n","$$Perplexity(X) = exp(-\\frac {1} {N} \\sum_{i}^{N} log P(x_i | x_{<i}))$$\n","\n","В этом задании нужно:\n","1. Посчитать вероятность **text**\n","2. Посчитать перплексию **text**\n","\n","Еще одна важная деталь:\n","работать с вероятностями плохо. Т.к. вероятность представляет собой число от 0 до 1, то при перемножении десятков или даже сотен таких числе теряется точность!\n","Для этого от произведения вероятностей берут логарифм и получают logprobs - логарифмы вероятностей. Их можно складывать, по свойству логарифма логарифм произведения равен произведению логарифма.\n","\n","$$ p = p_1 * p_2 * p_3 $$\n","$$log(p) = log (p_1) + log (p_2) + log (p_3)$$\n","$$exp(log (p)) = p = exp(log (p_1) + log (p_2) + log (p_3)) = exp (log (p_1 * p_2 * p_3)) = p_1 * p_2 * p_3$$\n","\n","В pytorch для этого есть `torch.log_softmax`, который считается численно стабильно!"]},{"cell_type":"code","execution_count":null,"id":"e1c7ba39-a451-43a2-ac55-629c99259abe","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e1c7ba39-a451-43a2-ac55-629c99259abe","executionInfo":{"status":"ok","timestamp":1727797185419,"user_tz":-180,"elapsed":666,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"bf4d1fed-00d2-4573-bb2f-bfab9098e310"},"outputs":[{"output_type":"stream","name":"stdout","text":["Beginning of sentence (BOS) token = `<|endoftext|>`\n","End of sentence (EOS) token  = `<|endoftext|>`\n","Probability of the text: 2.178365e-14\n","Perplexity of the text: 51.019203\n"]}],"source":["print(f\"Beginning of sentence (BOS) token = `{tokenizer.bos_token}`\")\n","print(f\"End of sentence (EOS) token  = `{tokenizer.eos_token}`\")\n","text = \"<|endoftext|>I'm so very tired of this<|endoftext|>\"\n","\n","# Устанавливаем add_special_tokens=False, чтобы явно контролировать специальные токены\n","inputs = tokenizer(text, add_special_tokens=False, return_tensors='pt')\n","\n","# Перенос входных данных на устройство\n","input_ids = inputs.input_ids.to(device)\n","\n","with torch.no_grad():\n","    logits = model(input_ids).logits\n","\n","    # ваш код здесь!\n","    # 1. Нужно обрезать logits по длине, т.к. для предсказаний по последнему токену нечего считать\n","    logits = logits[:, :-1, :]  # Форма: [1, seq_len - 1, vocab_size]\n","    # 2. Превращаем logits в log_probs\n","    log_probs = torch.log_softmax(logits, dim=-1)  # Форма: [1, seq_len - 1, vocab_size]\n","\n","    # 3. Берем вероятности следующих токенов, т.к. по вектору i-й позиции мы предсказываем токен на позиции (i + 1)\n","    targets = input_ids[:, 1:]  # Форма: [1, seq_len - 1]\n","    # для этого нам поможет torch.gather\n","    log_probs_of_targets = log_probs.gather(dim=-1, index=targets.unsqueeze(-1)).squeeze(-1)  # Форма: [1, seq_len -1]\n","\n","    # 4. Считаем вероятности и perplexity!\n","    sum_log_probs = log_probs_of_targets.sum()  # Скаляр\n","\n","    # Вычисляем вероятность текста путем экспоненцирования суммы логарифмов\n","    probability = torch.exp(sum_log_probs).item()\n","\n","    # Вычисляем перплексию по формуле\n","    N = targets.size(1)  # Количество предсказаний\n","    perplexity = np.exp(-sum_log_probs.item() / N)\n","\n","# должно получиться что-то около 2.1783e-14 для вероятности и около 51 для ppl\n","# Вывод результатов\n","print(f\"Probability of the text: {probability:.6e}\")\n","print(f\"Perplexity of the text: {perplexity:.6f}\")"]},{"cell_type":"markdown","id":"4f244eac-7cb1-4689-8adc-46662891e657","metadata":{"id":"4f244eac-7cb1-4689-8adc-46662891e657"},"source":["# Вопросы - 4 балла\n","\n","**Ответьте на вопрсоы текстом прямо здесь!**\n","\n","\n","1. Какое значение P(X) вероятности текста самое \"лучшее\" в том смысле, что модель максимально уверена в этом тексте и скорее всего его сгенерирует.\n","2. Какое значение перплексии текста самое \"лучшее\" в том смысле, что модель максимально уверена в этом тексте и скорее всего его сгенерирует.\n","\n"]},{"cell_type":"markdown","source":["1 - Лучшее значение вероятности 1, что означает максимальную уверенность модели в тексте.  \n","\n","2 - Лучшее значение перплексии также 1, что модель полностью уверена в предсказаниях каждого токена."],"metadata":{"id":"30w2l1ilvzin"},"id":"30w2l1ilvzin"},{"cell_type":"markdown","id":"5ddd5038-620b-48bb-bbc1-db3729141d78","metadata":{"id":"5ddd5038-620b-48bb-bbc1-db3729141d78"},"source":["# Chat-Models"]},{"cell_type":"markdown","id":"599c7530-a7ce-4d23-abaf-2b0cec87301e","metadata":{"id":"599c7530-a7ce-4d23-abaf-2b0cec87301e"},"source":["# Формат - 5 баллов\n","Как мы обсуждали на лекции, все chat-модели принимают входы в своем особом формате.\n","Он может быть описан текстом, а может быть заложен в шаблон, который доступен через `tokenizer.apply_chat_template`"]},{"cell_type":"code","execution_count":null,"id":"7f5fe593-63a8-406d-9678-6d805c180670","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":802,"referenced_widgets":["6a68cb86fd34463f8fd0d8f4fe18e898","71bb18417fe549799fd51f807a8aa676","1ec5eb6499bd48d4abc719ed016edeab","590f29af8cce4376a7f4b0fdf4e23973","e5bb1dc1c99e4d849dd7c103164cdbfa","c2c0f1565d934014bf4bbac42adf8590","12fb265a78234302b2a61655a3dd06be","539905d68df243c5a7a0714ea2b82efd","a4f3b229a90941068c3644388c7e83e4","09c127753bf0431489b552aa45578142","971423dc103547668b1d111ada652526","75fd9cc57b3649cdb33f29adaafd5c59","2af0a6ca60e54a1e83fc4c4fa7136a93","f709748b04da40bc9dc668ade058a765","3b1416ad196c44d092a24c7b2fd2ffb6","2eb8cd71b2b14585aa456a9dd5dc32ae","ee4dc0e3860f4957a083760509555643","e43e2d4afdaa42afa107d90d12abf20d","e8076168de26425381c16ec0825ce306","103524f5dee3476b81cc9c5948e0701b","289c1824f99f4a378e86c6bdc707ac9b","472de7e5cdc045598e3926c6c9a12c04","727f09c5c9354e36a993d2b77b5c60fb","d12196d1965e4ee393168b42728c9411","4faf4bfaa719408a8230a4be5c1e107c","5da149a893fb49c2aea6aa0c0edc994e","c6766675153f4bc38ac669deda4a7f96","edc3eb95c55a4c62994ee80cce2e36a4","7f6c5e5444b344f59eb799ba1056b574","916e4b340a8a42a9a0baf15e741da885","ba7f1891f27a47608cad6d2bcb900320","ac3d81ed0e084574b7674e5ca294d437","5c128e4641e9454dbd6760dacc1d4aa7","d7781382d1344bfb9dfcffb2ac56b40b","bd84a8cd882b4b8b9a13c6fc87e984ec","c43c9f95299e4144b840314df0ed5fae","01f730df4a0a4efaaff01e6edbc6b3b7","f66f4b393d2445d3a41362e4abfde268","7722c90819fd4d22aeb144dd4087224a","af979175093347e48c6c2ad73ff6c51b","ed0f53bca8aa44e3aab9160f67011f7b","3a916ed4fc334ec4b86afac26c76f2bc","c20827bbfa044b37abfc5456a2bbaaf3","0301106760a44bfe87c88c69dc4caf8f","a688b0c02f694451b733aafc58c4c62a","a28fc0d9537b44efbd40e5fcf90b4f52","1cbfbc728db446b6a3d5755c21f6d256","9d5e37ddd7694eeb8a9e0918222e84c2","dfd4890de50d40dfb17b75e22288ab87","e2af9a66a91644a1b7cc853069bb2679","0e843781bcb548d6ad00b3279a7da7d5","e6a90859bb0541f39d39e1acb16120ba","893191d46e0345a393da7a64dceab216","3afd5a1226214fa58874a4fa3bb0aa8b","46b3388eed1242b69e39c895e21cfdd5","b3afac7a80664cadbf645b4c348e06e9","9b8e4f5582f042489b9b2524f858a401","3ab02b25228e4f738fdd454f263bb5f5","15b3a6a0bfa04a608486cdd2d69090cc","2e2b094dae3f49acbe075996df3fb649","94510b75c6294288a455e54d18b2d72a","8b8c011b6d6b483292e99a806df6cf85","3c573ba2eff74d5fb4702f4fd5d65197","95cffa8bc3774262bde0ceeef991f6e1","e783c6b8a2b94b81857f2c8598173bf6","028b23158a0e455d9987b365679a7d35"]},"id":"7f5fe593-63a8-406d-9678-6d805c180670","executionInfo":{"status":"ok","timestamp":1727802900825,"user_tz":-180,"elapsed":61628,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"8ba12c2c-96f5-43ab-860f-4cd04cc9d2c2"},"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a68cb86fd34463f8fd0d8f4fe18e898"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75fd9cc57b3649cdb33f29adaafd5c59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"727f09c5c9354e36a993d2b77b5c60fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7781382d1344bfb9dfcffb2ac56b40b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a688b0c02f694451b733aafc58c4c62a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/5.31G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3afac7a80664cadbf645b4c348e06e9"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["GPTNeoForCausalLM(\n","  (transformer): GPTNeoModel(\n","    (wte): Embedding(50257, 2048)\n","    (wpe): Embedding(2048, 2048)\n","    (drop): Dropout(p=0.0, inplace=False)\n","    (h): ModuleList(\n","      (0-23): 24 x GPTNeoBlock(\n","        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPTNeoAttention(\n","          (attention): GPTNeoSelfAttention(\n","            (attn_dropout): Dropout(p=0.0, inplace=False)\n","            (resid_dropout): Dropout(p=0.0, inplace=False)\n","            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n","          )\n","        )\n","        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPTNeoMLP(\n","          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n","          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":4}],"source":["model_name = \"EleutherAI/gpt-neo-1.3B\" #\"NousResearch/Meta-Llama-3-8B-Instruct\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token_id = tokenizer.eos_token_id  # Устанавливаем pad_token_id\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    torch_dtype=torch.half,          # Используем 16-битную точность для экономии памяти\n","    #low_cpu_mem_usage=True           # Оптимизация использования памяти на CPU\n",").to(device)\n","model.eval()  # Переводим модель в режим оценки"]},{"cell_type":"code","execution_count":null,"id":"7d8dad11-6811-49ab-ad3d-8ac7de103828","metadata":{"id":"7d8dad11-6811-49ab-ad3d-8ac7de103828"},"outputs":[],"source":["def move_to_device(d):\n","    for k, v in d.items():\n","        d[k] = v.to(device)\n","    return d"]},{"cell_type":"markdown","id":"503f15fd-576a-4e8e-917a-74df01a944f4","metadata":{"id":"503f15fd-576a-4e8e-917a-74df01a944f4"},"source":["Давайте посмотрим, как chat модель отработает на обычном тексте. Используйте для генерации сэмплинг и kv cache, выведите 5 результатов генерации."]},{"cell_type":"code","source":["# Исходный текст для генерации\n","text = \"hello how are you\"\n","\n","# Токенизируем входной текст\n","inputs = tokenizer(text, return_tensors='pt')\n","inputs = move_to_device(inputs)  # Переносим входные данные на устройство\n","\n","# Генерируем и выводим 5 вариантов продолжения\n","for i in range(5):\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            **inputs,\n","            max_new_tokens=50,       # Максимальное количество генерируемых токенов\n","            do_sample=True,          # Включаем сэмплинг\n","            temperature=0.7,         # Температура для сэмплинга (регулирует разнообразие)\n","            top_p=0.9,               # Ядерное сэмплирование\n","            num_return_sequences=1,  # Количество возвращаемых последовательностей\n","            use_cache=True           # Используем kv cache для ускорения\n","        )\n","\n","    # Декодируем сгенерированные токены в текст\n","    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    print(f\"Генерация {i+1}:\")\n","    print(generated_text)\n","    print(\"===\" * 5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JOE9YnleBmCh","executionInfo":{"status":"ok","timestamp":1727803524272,"user_tz":-180,"elapsed":616526,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"248ca80d-66f3-4039-ce4d-5bd854ec07f5"},"id":"JOE9YnleBmCh","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Генерация 1:\n","hello how are you\n","all good how are you all\n","alright thank you for having\n","me here today I'm here today\n","with my good friend and fellow\n","writer and author of the book\n","\"The Art of Writing\" he is\n","currently writing his second\n","===============\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Генерация 2:\n","hello how are you doing today?\n","I am doing good,\n","I am doing good,\n","I am doing good.\n","I am doing good,\n","I am doing good,\n","I am doing good.\n","I am doing good,\n","I am doing good\n","===============\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Генерация 3:\n","hello how are you?\n","i'm fine\n","what's your name?\n","my name is jim\n","hello jim\n","how are you?\n","i'm fine\n","what's your name?\n","my name is jim\n","hello jim\n","how are you\n","===============\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Генерация 4:\n","hello how are you\n","my name is lisa i'm a\n","fellow member of the\n","council of eu we're\n","a group of scientists\n","who study the origins\n","of life and\n","the history of life\n","and the origins of life\n","\n","===============\n","Генерация 5:\n","hello how are you?\" \"I'm fine.\" \"I was just wondering if you had a minute?\" \"Sure, I can squeeze you in.\" \"Okay.\" \"Oh, God, I was just thinking.\" \"It's weird, I'm not used to being\n","===============\n"]}]},{"cell_type":"markdown","id":"3fd50470-64b9-4a21-8748-0e9c5ea439fc","metadata":{"id":"3fd50470-64b9-4a21-8748-0e9c5ea439fc"},"source":["Видим, что текст зачастую выходит мусорный. Это потому что формат входных данных сильно отличается от того, что модель видела на обучении.\n","Как мы уже обсуждали, у всех chat-моделей свой формат. Где-то он описан просто словами, где-то он заложен в токенайзер. Мы рассмотрим как раз такой случай - за нас есть удобно написанная функция `apply_chat_template`. Давайте используем ее, чтобы получить префикс для генерации модели.\n","\n","Не забудьте про опцию add_generation_prefix - она добавляет часть формата, после которой ожидается ответ модели!"]},{"cell_type":"code","execution_count":null,"id":"e79a3701-c80f-4b90-90bd-fa010e32ea36","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"e79a3701-c80f-4b90-90bd-fa010e32ea36","executionInfo":{"status":"error","timestamp":1727803681070,"user_tz":-180,"elapsed":722,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"ef57beed-a276-49ad-f161-8eac85ccf187"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-a03958e33f29>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_generation_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m reference = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mapply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m             \u001b[0mtokenizer_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1786\u001b[0;31m         \u001b[0mchat_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_assistant_tokens_mask\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\{\\%-?\\s*generation\\s*-?\\%\\}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mget_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2024\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2025\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   2026\u001b[0m                     \u001b[0;34m\"Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2027\u001b[0m                     \u001b[0;34m\"argument was passed! For information about writing templates and setting the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"]}],"source":["messages = [\n","    {\"role\": \"user\", \"content\": \"hello\"},\n","    {\"role\": \"assistant\", \"content\": \"I'm good. How can I help you today\"},\n","    {\"role\": \"user\", \"content\": \"I love you\"},\n","]\n","\n","prefix = tokenizer.apply_chat_template(tokenizer, messages, add_generation_prefix=True)\n","\n","reference = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n","\n","hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","I'm good. How can I help you today<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","I love you<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n","\n","assert prefix.strip() == reference.strip()"]},{"cell_type":"markdown","source":["Пробуем выкрутиться!!!!!"],"metadata":{"id":"oLhzSlpeJr9t"},"id":"oLhzSlpeJr9t"},{"cell_type":"code","source":["from transformers import PreTrainedTokenizerBase\n","\n","# Определяем собственный шаблон чата\n","class SimpleChatTemplate:\n","    def build_prompt(self, messages: List[dict], add_generation_prefix: bool = True) -> str:\n","        return format_messages(messages)\n","\n","def format_messages(messages: List[dict]) -> str:\n","    \"\"\"\n","    Форматирует список сообщений в строку-промпт для модели.\n","\n","    Args:\n","        messages (List[dict]): Список сообщений, каждое из которых является словарём с ключами 'role' и 'content'.\n","\n","    Returns:\n","        str: Форматированный строковый промпт.\n","    \"\"\"\n","    formatted = \"\"\n","    for message in messages:\n","        role = message.get(\"role\", \"\").capitalize()\n","        content = message.get(\"content\", \"\")\n","        if role and content:\n","            formatted += f\"{role}: {content}\\n\"\n","    # Добавляем следующую роль для генерации ответа\n","    formatted += \"assistant:\"\n","    return formatted\n"],"metadata":{"id":"B6paEcuhIUJt"},"id":"B6paEcuhIUJt","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Присваиваем шаблон токенизатору\n","tokenizer.chat_template = SimpleChatTemplate()\n","\n","messages = [\n","    {\"role\": \"user\", \"content\": \"hello\"},\n","    {\"role\": \"assistant\", \"content\": \"I'm good. How can I help you today\"},\n","    {\"role\": \"user\", \"content\": \"I love you\"},\n","]\n","\n","\n","# Теперь можно использовать apply_chat_template\n","prefix = tokenizer.chat_template.build_prompt(messages)\n","\n","\n","reference = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n","\n","hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","I'm good. How can I help you today<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","I love you<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n","\n","assert prefix.strip() == reference.strip()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"id":"ooIlDoyQIxYo","executionInfo":{"status":"error","timestamp":1727803975796,"user_tz":-180,"elapsed":318,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"b581d109-7eef-4603-a0d1-b9b4213a0e9d"},"id":"ooIlDoyQIxYo","execution_count":null,"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-bd77fa2fa245>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m I love you<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAssertionError\u001b[0m: "]}]},{"cell_type":"markdown","source":["Это не инструктивная модель и в ней нет нужных инструкций!"],"metadata":{"id":"8X1Ae0CDJPnN"},"id":"8X1Ae0CDJPnN"},{"cell_type":"code","source":["prefix.strip() , reference.strip()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zbj56Cc9JJ2N","executionInfo":{"status":"ok","timestamp":1727803995469,"user_tz":-180,"elapsed":350,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"05384474-5e37-4fcd-d840-4a84a2338238"},"id":"zbj56Cc9JJ2N","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(\"User: hello\\nAssistant: I'm good. How can I help you today\\nUser: I love you\\nassistant:\",\n"," \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nhello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI'm good. How can I help you today<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nI love you<|eot_id|><|start_header_id|>assistant<|end_header_id|>\")"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","id":"048b8882-bec0-4bf4-b6fb-30e727d095c6","metadata":{"id":"048b8882-bec0-4bf4-b6fb-30e727d095c6"},"source":["Давайте посмотрим, что нам ответит модель!"]},{"cell_type":"code","source":["inputs = tokenizer(prefix, return_tensors=\"pt\")\n","\n","# Список для хранения результатов генерации\n","batched_generations: List[str] = []\n","\n","# Генерация и вывод результатов\n","with torch.no_grad():\n","    for i in range(5):\n","        # Генерация текста\n","        outputs = model.generate(\n","            input_ids=inputs[\"input_ids\"],\n","            attention_mask=inputs[\"attention_mask\"],\n","            max_new_tokens=50,       # Максимальное количество генерируемых токенов\n","            do_sample=True,          # Включаем сэмплинг\n","            temperature=0.7,         # Температура для сэмплинга (регулирует разнообразие)\n","            top_p=0.9,               # Ядерное сэмплирование\n","            num_return_sequences=1,  # Количество возвращаемых последовательностей\n","            use_cache=True           # Используем kv cache для ускорения\n","        )\n","        # Декодирование сгенерированных токенов\n","        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        batched_generations.append(generated_text)\n","\n","        # Вывод сгенерированного текста\n","        print(generated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F5LZYKYE54SX","executionInfo":{"status":"ok","timestamp":1727804985090,"user_tz":-180,"elapsed":894222,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"6bee96a1-d733-46ab-fb17-2c631ae4927e"},"id":"F5LZYKYE54SX","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["User: hello\n","Assistant: I'm good. How can I help you today\n","User: I love you\n","assistant: I'm good. How can I help you today\n","User: I love you\n","assistant: I'm good. How can I help you today\n","User: I love you\n","assistant: I'm good. How can I help you today\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["User: hello\n","Assistant: I'm good. How can I help you today\n","User: I love you\n","assistant: I'm good. How can I help you today\n","User: I love you\n","assistant: I'm good. How can I help you today\n","User: I love you\n","assistant: I'm good. How can I help you today\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["User: hello\n","Assistant: I'm good. How can I help you today\n","User: I love you\n","assistant: I love you too.\n","User: I'm not sure what you mean\n","assistant: I don't know what you mean.\n","User: I'm not sure what you mean\n","assistant: I don't know what you mean.\n","\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["User: hello\n","Assistant: I'm good. How can I help you today\n","User: I love you\n","assistant: I'm good. How can I help you today\n","User: I love you\n","assistant: I'm good. How can I help you today\n","User: I love you\n","assistant: I'm good. How can I help you today\n","User: hello\n","Assistant: I'm good. How can I help you today\n","User: I love you\n","assistant: I'm good. How can I help you today\n","User: I love you\n","assistant: I'm good. How can I product\n","User: I love you\n","assistant: I'm good. How can I product\n","User: I\n"]}]},{"cell_type":"markdown","source":["-----------------\n","\n","- Далее вероятно уже не успею, поэтому до основного дедлайна сдаю следующего пункта!\n","\n","- Если найду время, то постараюсь доделать концовку к мягкому дедлайну."],"metadata":{"id":"UOYn9ZRPJ2bi"},"id":"UOYn9ZRPJ2bi"},{"cell_type":"markdown","id":"a72482f3-c296-46f3-851c-57b4f91a717b","metadata":{"id":"a72482f3-c296-46f3-851c-57b4f91a717b"},"source":["## Benchmark - 15 баллов"]},{"cell_type":"markdown","id":"52f422a9-c2ee-4c17-8aee-1830f1d143e6","metadata":{"id":"52f422a9-c2ee-4c17-8aee-1830f1d143e6"},"source":["Перед нами датасет MMLU - датасет вопросов и ответов в стиле multiple choice.\n","* question - вопрос\n","* choices - варианты ответа\n","* answer - номер правильного ответа"]},{"cell_type":"code","execution_count":null,"id":"530d1721-6623-4ca6-816c-d4f90203ceb2","metadata":{"id":"530d1721-6623-4ca6-816c-d4f90203ceb2"},"outputs":[],"source":["from datasets import load_dataset\n","mmlu = load_dataset(\"cais/mmlu\", \"global_facts\", split=\"test\")\n","mmlu[1]"]},{"cell_type":"markdown","id":"fca61a91-5784-44f3-af9b-72250f8d58a4","metadata":{"id":"fca61a91-5784-44f3-af9b-72250f8d58a4"},"source":["Наша задача здесь решить задачу многоклассовой классификации.\n","Для этого нужно посчитать\n","$$P(choices_i | question)$$\n","т.е. для посчитать вероятность каждого варианта ответа для вопроса. Мы это уже делали кодом выше!\n","\n","После этого давайте брать самый вероятный ответ и считать, что модель его выбрала.\n","После этого давайте посчитаем accurracy, т.е. долю правильных ответов.\n","Вместо вероятностей для подсчета лучше использовать logprobs.\n","\n","Итого, что нужно сделать:\n","1. Пройтись по датасету, для каждого question и каждого из соответствующих choices получить самый вероятный ответ.\n","2. Посчитать итоговый accuracy\n","\n","* За базовую реализацию с двойным циклом по questions и choices дается 5 баллов.\n","* еще 3 балла дается, если собрать question и все его choices в батч размера 4 и считать вероятности параллельно для всего батча сразу.\n","* еще 3 балла дается, если собрать несколько questions и их choces в батч размера 4 * num_questions_in_batch и считать вероятности параллельно для всего батча сразу.\n","\n","Что важно помнить:\n","1. Данные нужно подавать в правильном формате для llama3!\n","2. Если делаете варианты с батчеванием помните: длины choices могут быть разными! Нужно не считать вероятности по паддингам. В этом нам помогут attention_masks из выходов `tokenizer()`"]},{"cell_type":"code","execution_count":null,"id":"4a4126e2-c463-404d-a3b5-5361f744242e","metadata":{"id":"4a4126e2-c463-404d-a3b5-5361f744242e"},"outputs":[],"source":["# ваш код здесь!"]},{"cell_type":"code","execution_count":null,"id":"9fe9c63f-5d9e-4204-b1a9-d93ca19c1888","metadata":{"id":"9fe9c63f-5d9e-4204-b1a9-d93ca19c1888"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"76c440af-dfc0-460d-a113-3fab3fefa361","metadata":{"id":"76c440af-dfc0-460d-a113-3fab3fefa361"},"source":["**Ответьте на следующие вопросы (2 балла за вопрос)**:\n","1. Как влияет длина ответа на вероятность ответа при скоринге? Если есть какие-либо проблемы, как бы вы с этим боролись.\n","2. Если к началу каждого ответа добавилить метки A) B) C) D) станет ли модель отвечать лучше или хуже?\n","Стоит ли по-вашему добавлять эти метки?\n"]},{"cell_type":"code","execution_count":null,"id":"3835f1a9-05fb-440f-93b4-01c53419eb96","metadata":{"id":"3835f1a9-05fb-440f-93b4-01c53419eb96"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"6a68cb86fd34463f8fd0d8f4fe18e898":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_71bb18417fe549799fd51f807a8aa676","IPY_MODEL_1ec5eb6499bd48d4abc719ed016edeab","IPY_MODEL_590f29af8cce4376a7f4b0fdf4e23973"],"layout":"IPY_MODEL_e5bb1dc1c99e4d849dd7c103164cdbfa"}},"71bb18417fe549799fd51f807a8aa676":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2c0f1565d934014bf4bbac42adf8590","placeholder":"​","style":"IPY_MODEL_12fb265a78234302b2a61655a3dd06be","value":"tokenizer_config.json: 100%"}},"1ec5eb6499bd48d4abc719ed016edeab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_539905d68df243c5a7a0714ea2b82efd","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a4f3b229a90941068c3644388c7e83e4","value":200}},"590f29af8cce4376a7f4b0fdf4e23973":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09c127753bf0431489b552aa45578142","placeholder":"​","style":"IPY_MODEL_971423dc103547668b1d111ada652526","value":" 200/200 [00:00&lt;00:00, 3.54kB/s]"}},"e5bb1dc1c99e4d849dd7c103164cdbfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2c0f1565d934014bf4bbac42adf8590":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12fb265a78234302b2a61655a3dd06be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"539905d68df243c5a7a0714ea2b82efd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4f3b229a90941068c3644388c7e83e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"09c127753bf0431489b552aa45578142":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"971423dc103547668b1d111ada652526":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75fd9cc57b3649cdb33f29adaafd5c59":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2af0a6ca60e54a1e83fc4c4fa7136a93","IPY_MODEL_f709748b04da40bc9dc668ade058a765","IPY_MODEL_3b1416ad196c44d092a24c7b2fd2ffb6"],"layout":"IPY_MODEL_2eb8cd71b2b14585aa456a9dd5dc32ae"}},"2af0a6ca60e54a1e83fc4c4fa7136a93":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee4dc0e3860f4957a083760509555643","placeholder":"​","style":"IPY_MODEL_e43e2d4afdaa42afa107d90d12abf20d","value":"config.json: 100%"}},"f709748b04da40bc9dc668ade058a765":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8076168de26425381c16ec0825ce306","max":1347,"min":0,"orientation":"horizontal","style":"IPY_MODEL_103524f5dee3476b81cc9c5948e0701b","value":1347}},"3b1416ad196c44d092a24c7b2fd2ffb6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_289c1824f99f4a378e86c6bdc707ac9b","placeholder":"​","style":"IPY_MODEL_472de7e5cdc045598e3926c6c9a12c04","value":" 1.35k/1.35k [00:00&lt;00:00, 17.9kB/s]"}},"2eb8cd71b2b14585aa456a9dd5dc32ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee4dc0e3860f4957a083760509555643":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e43e2d4afdaa42afa107d90d12abf20d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8076168de26425381c16ec0825ce306":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"103524f5dee3476b81cc9c5948e0701b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"289c1824f99f4a378e86c6bdc707ac9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"472de7e5cdc045598e3926c6c9a12c04":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"727f09c5c9354e36a993d2b77b5c60fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d12196d1965e4ee393168b42728c9411","IPY_MODEL_4faf4bfaa719408a8230a4be5c1e107c","IPY_MODEL_5da149a893fb49c2aea6aa0c0edc994e"],"layout":"IPY_MODEL_c6766675153f4bc38ac669deda4a7f96"}},"d12196d1965e4ee393168b42728c9411":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_edc3eb95c55a4c62994ee80cce2e36a4","placeholder":"​","style":"IPY_MODEL_7f6c5e5444b344f59eb799ba1056b574","value":"vocab.json: 100%"}},"4faf4bfaa719408a8230a4be5c1e107c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_916e4b340a8a42a9a0baf15e741da885","max":798156,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ba7f1891f27a47608cad6d2bcb900320","value":798156}},"5da149a893fb49c2aea6aa0c0edc994e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac3d81ed0e084574b7674e5ca294d437","placeholder":"​","style":"IPY_MODEL_5c128e4641e9454dbd6760dacc1d4aa7","value":" 798k/798k [00:00&lt;00:00, 4.56MB/s]"}},"c6766675153f4bc38ac669deda4a7f96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edc3eb95c55a4c62994ee80cce2e36a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f6c5e5444b344f59eb799ba1056b574":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"916e4b340a8a42a9a0baf15e741da885":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba7f1891f27a47608cad6d2bcb900320":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ac3d81ed0e084574b7674e5ca294d437":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c128e4641e9454dbd6760dacc1d4aa7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7781382d1344bfb9dfcffb2ac56b40b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bd84a8cd882b4b8b9a13c6fc87e984ec","IPY_MODEL_c43c9f95299e4144b840314df0ed5fae","IPY_MODEL_01f730df4a0a4efaaff01e6edbc6b3b7"],"layout":"IPY_MODEL_f66f4b393d2445d3a41362e4abfde268"}},"bd84a8cd882b4b8b9a13c6fc87e984ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7722c90819fd4d22aeb144dd4087224a","placeholder":"​","style":"IPY_MODEL_af979175093347e48c6c2ad73ff6c51b","value":"merges.txt: 100%"}},"c43c9f95299e4144b840314df0ed5fae":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed0f53bca8aa44e3aab9160f67011f7b","max":456356,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3a916ed4fc334ec4b86afac26c76f2bc","value":456356}},"01f730df4a0a4efaaff01e6edbc6b3b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c20827bbfa044b37abfc5456a2bbaaf3","placeholder":"​","style":"IPY_MODEL_0301106760a44bfe87c88c69dc4caf8f","value":" 456k/456k [00:00&lt;00:00, 5.72MB/s]"}},"f66f4b393d2445d3a41362e4abfde268":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7722c90819fd4d22aeb144dd4087224a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af979175093347e48c6c2ad73ff6c51b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed0f53bca8aa44e3aab9160f67011f7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a916ed4fc334ec4b86afac26c76f2bc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c20827bbfa044b37abfc5456a2bbaaf3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0301106760a44bfe87c88c69dc4caf8f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a688b0c02f694451b733aafc58c4c62a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a28fc0d9537b44efbd40e5fcf90b4f52","IPY_MODEL_1cbfbc728db446b6a3d5755c21f6d256","IPY_MODEL_9d5e37ddd7694eeb8a9e0918222e84c2"],"layout":"IPY_MODEL_dfd4890de50d40dfb17b75e22288ab87"}},"a28fc0d9537b44efbd40e5fcf90b4f52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2af9a66a91644a1b7cc853069bb2679","placeholder":"​","style":"IPY_MODEL_0e843781bcb548d6ad00b3279a7da7d5","value":"special_tokens_map.json: 100%"}},"1cbfbc728db446b6a3d5755c21f6d256":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6a90859bb0541f39d39e1acb16120ba","max":90,"min":0,"orientation":"horizontal","style":"IPY_MODEL_893191d46e0345a393da7a64dceab216","value":90}},"9d5e37ddd7694eeb8a9e0918222e84c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3afd5a1226214fa58874a4fa3bb0aa8b","placeholder":"​","style":"IPY_MODEL_46b3388eed1242b69e39c895e21cfdd5","value":" 90.0/90.0 [00:00&lt;00:00, 1.43kB/s]"}},"dfd4890de50d40dfb17b75e22288ab87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2af9a66a91644a1b7cc853069bb2679":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e843781bcb548d6ad00b3279a7da7d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6a90859bb0541f39d39e1acb16120ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"893191d46e0345a393da7a64dceab216":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3afd5a1226214fa58874a4fa3bb0aa8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46b3388eed1242b69e39c895e21cfdd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3afac7a80664cadbf645b4c348e06e9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9b8e4f5582f042489b9b2524f858a401","IPY_MODEL_3ab02b25228e4f738fdd454f263bb5f5","IPY_MODEL_15b3a6a0bfa04a608486cdd2d69090cc"],"layout":"IPY_MODEL_2e2b094dae3f49acbe075996df3fb649"}},"9b8e4f5582f042489b9b2524f858a401":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94510b75c6294288a455e54d18b2d72a","placeholder":"​","style":"IPY_MODEL_8b8c011b6d6b483292e99a806df6cf85","value":"model.safetensors: 100%"}},"3ab02b25228e4f738fdd454f263bb5f5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c573ba2eff74d5fb4702f4fd5d65197","max":5312673800,"min":0,"orientation":"horizontal","style":"IPY_MODEL_95cffa8bc3774262bde0ceeef991f6e1","value":5312673800}},"15b3a6a0bfa04a608486cdd2d69090cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e783c6b8a2b94b81857f2c8598173bf6","placeholder":"​","style":"IPY_MODEL_028b23158a0e455d9987b365679a7d35","value":" 5.31G/5.31G [00:37&lt;00:00, 163MB/s]"}},"2e2b094dae3f49acbe075996df3fb649":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94510b75c6294288a455e54d18b2d72a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b8c011b6d6b483292e99a806df6cf85":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c573ba2eff74d5fb4702f4fd5d65197":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95cffa8bc3774262bde0ceeef991f6e1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e783c6b8a2b94b81857f2c8598173bf6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"028b23158a0e455d9987b365679a7d35":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}