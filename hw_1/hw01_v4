{"cells":[{"cell_type":"markdown","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%% md\n"},"id":"qzVMLmXCKeh2"},"source":["# –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ 1 (50 –±–∞–ª–ª–æ–≤)\n","v4"]},{"cell_type":"markdown","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%% md\n"},"id":"XjT0WR1YKeh8"},"source":["–í —ç—Ç–æ–º –¥–æ–º–∞—à–Ω–µ–º –∑–∞–¥–∞–Ω–∏–∏ –≤—ã –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç–µ—Å—å —Å –æ—Å–Ω–æ–≤–∞–º–∏ NLP, –Ω–∞—É—á–∏—Ç–µ—Å—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ç–µ–∫—Å—Ç—ã.\n","\n","–ë—É–¥–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤, –æ–±–æ–∑–Ω–∞—á–µ–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–º ü§î.\n","\n","–ë–ª–æ–∫–∏, –≥–¥–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –Ω–∞–ø–∏—Å–∞—Ç—å —Å–≤–æ–π –∫–æ–¥, –æ–±–æ–∑–Ω–∞—á–µ–Ω—ã —á–µ—Ä–µ–∑ YOUR_CODE_HERE. –ö–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —É–∂–µ –Ω–∞–ø–∏—Å–∞–Ω, –º–µ–Ω—è—Ç—å –Ω–µ –Ω—É–∂–Ω–æ. –ë–ª–æ–∫–∏ –≤ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –∑–∞–¥–∞–Ω–∏—è –∑–∞–≤–∏—Å—è—Ç –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞, –ø–æ—ç—Ç–æ–º—É –Ω–µ –≤–Ω–æ—Å–∏—Ç–µ –±–æ–ª—å—à–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π, —á—Ç–æ–±—ã –Ω–∏—á–µ–≥–æ –Ω–µ —Å–ª–æ–º–∞–ª–æ—Å—å."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"scrolled":true,"id":"vK4xB1J4Keh-"},"outputs":[],"source":["!pip install gensim nltk torch tqdm seqeval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sM1aTzsfKeiB"},"outputs":[],"source":["from typing import List, Dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DgTcMr1KKeiB"},"outputs":[],"source":["YOUR_CODE_HERE = None  # –∑–∞–≥–ª—É—à–∫–∞, –∑–¥–µ—Å—å –Ω–∏—á–µ–≥–æ –Ω–µ —Ç—Ä–æ–≥–∞–π—Ç–µ"]},{"cell_type":"markdown","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"id":"YzlD7DrXKeiC"},"source":["## –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (5 –±–∞–ª–ª–æ–≤)"]},{"cell_type":"markdown","metadata":{"id":"it9D6AVyKeiC"},"source":["–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è - —ç—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –Ω–∞–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤.\n","–ù–∞–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ –ø—Ä–æ–±–µ–ª–∞–º. –ë–æ–ª–µ–µ —É–º–Ω—ã–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —É—á–∏—Ç—ã–≤–∞—é—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é.\n","\n","–ù–∞—É—á–∏–º—Å—è —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π NLTK, –≥–¥–µ —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ —Ä–∞–±–æ—Ç–∞ —Å –ø—É–Ω–∫—Ç—É–∞—Ü–∏–µ–π.\n","\n","https://www.nltk.org/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3FdxssYxKeiD"},"outputs":[],"source":["import nltk\n","\n","# https://www.nltk.org/nltk_data/\n","nltk.download(\"punkt\")\n","nltk.download('punkt_tab')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t1kzCaFzKeiE"},"outputs":[],"source":["def tokenize(text: str, language: str = \"english\", lower: bool = False) -> List[str]:\n","    # YOUR_CODE_HERE\n","    ...\n","\n","\n","assert tokenize(\"\") == []\n","assert tokenize(\"Hello, world!\") == [\"Hello\", \",\", \"world\", \"!\"]\n","assert tokenize(\"EU rejects German call to boycott British lamb.\") == [\"EU\", \"rejects\", \"German\", \"call\", \"to\", \"boycott\", \"British\", \"lamb\", \".\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QsxLUV2ZKeiF"},"outputs":[],"source":["def split_sentences(text: str, language: str = \"english\", lower: bool = False) -> List[str]:\n","    # YOUR_CODE_HERE\n","    ...\n","\n","\n","assert split_sentences(\"\") == []\n","assert split_sentences(\"Hello, world!\") == [\"Hello, world!\"]\n","assert split_sentences(\"Hello, world! I love Python!\") == [\"Hello, world!\", \"I love Python!\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h7zOBxZ0KeiF"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"1KCLb7xFKeiF"},"source":["## –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (5 –±–∞–ª–ª–æ–≤)"]},{"cell_type":"markdown","metadata":{"id":"UawIRDChKeiG"},"source":["–ß—Ç–æ–±—ã —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ç–µ–∫—Å—Ç–∞–º–∏, –Ω—É–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω—ã (—Å–ª–æ–≤–∞) –≤ –≤–µ–∫—Ç–æ—Ä—ã.\n","–î–ª—è —ç—Ç–æ–≥–æ —Å–Ω–∞—á–∞–ª–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–æ—Å—Ç–∞–≤–∏—Ç—å —Å–ª–æ–≤–∞—Ä—å –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∂–¥–æ–º—É —Å–ª–æ–≤—É —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å.\n","–í –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–ª—è—Ö —Ä–µ–∞–ª–∏–∑—É–µ–º —ç—Ç–æ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –Ω—É–ª—è."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9kF-9ARKeiG"},"outputs":[],"source":["from collections import defaultdict, Counter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gs97PblZKeiG"},"outputs":[],"source":["class Vocabulary:\n","    def __init__(self, texts: List[str], language: str = \"english\", min_count: int = 1, lower: bool = False):\n","        \"\"\"\n","        –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è.\n","\n","        :param texts: –∫–æ–ª–ª–µ–∫—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤\n","        :param language: —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–æ–≤\n","        :param min_count: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–ø–∞–¥–∞–Ω–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å\n","        \"\"\"\n","        self.language = language\n","        self.lower = lower\n","        self.min_count = min_count\n","        self.word2idx = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n","        self.idx2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n","        self.word2count = Counter()\n","        self._build_vocabulary(texts)\n","\n","    def _build_vocabulary(self, texts: List[str]):\n","        for text in texts:\n","            # YOUR_CODE_HERE\n","            ...\n","\n","        for word, count in self.word2count.items():\n","            # YOUR_CODE_HERE\n","            ...\n","\n","    def encode_word(self, text: str) -> int:\n","        if self.lower:\n","            text = text.lower()\n","        return self.word2idx.get(text, self.word2idx[\"<UNK>\"])\n","\n","    def encode(self, text: str) -> List[int]:\n","        \"\"\"\n","        –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –Ω–∞–±–æ—Ä –∏–Ω–¥–µ–∫—Å–æ–≤.\n","\n","        :param text: —Ç–µ–∫—Å—Ç\n","        :return: –Ω–∞–±–æ—Ä –∏–Ω–¥–µ–∫—Å–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤\n","        \"\"\"\n","        # YOUR_CODE_HERE\n","        ...\n","\n","    def decode(self, input_ids: List[int]) -> str:\n","        \"\"\"\n","        –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–±–æ—Ä–∞ –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ —Ç–µ–∫—Å—Ç.\n","\n","        :param input_ids: –Ω–∞–±–æ—Ä –∏–Ω–¥–µ–∫—Å–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤\n","        :return: —Ç–µ–∫—Å—Ç\n","        \"\"\"\n","        return \" \".join([self.idx2word[idx] for idx in input_ids if idx != self.word2idx[\"<PAD>\"]])\n","\n","    def __len__(self):\n","        return len(self.word2idx)\n","\n","    def __contains__(self, item):\n","        return item in self.word2idx\n","\n","    def __iter__(self):\n","        return iter(self.word2idx)\n","\n","    def __str__(self):\n","        return str(self.word2idx)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lA9H9r_EKeiH"},"outputs":[],"source":["vocab = Vocabulary([\"Hello, world!\", \"I love Python!\"], min_count=1, lower=True)\n","encoded = vocab.encode(\"Hello, Python! I love you\")\n","assert vocab.decode(encoded) == \"hello , python ! i love <UNK>\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DESCXH6uKeiH"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"c7Vgq-UzKeiH"},"source":["## $n$-–≥—Ä–∞–º–º–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (10 –±–∞–ª–ª–æ–≤)"]},{"cell_type":"markdown","metadata":{"id":"MenRO83jKeiI"},"source":["–ù–∞–ø–∏—à–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é $n$-–≥—Ä–∞–º–º–Ω—É—é –º–æ–¥–µ–ª—å."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rdvAofqoKeiI"},"outputs":[],"source":["class NGramLanguageModel:\n","    def __init__(self, n: int, vocabulary: Vocabulary, texts: List[str]):\n","        \"\"\"\n","        –°–æ–∑–¥–∞–Ω–∏–µ n-–≥—Ä–∞–º–º–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏.\n","\n","        :param n: –ø–æ—Ä—è–¥–æ–∫ n-–≥—Ä–∞–º–º\n","        :param vocabulary: —Å–ª–æ–≤–∞—Ä—å\n","        \"\"\"\n","        assert n >= 2\n","        self.n = n\n","        self.vocabulary = vocabulary\n","        self.frequencies = defaultdict(lambda: Counter())  # —á–∞—Å—Ç–æ—Ç–∞ n-–≥—Ä–∞–º–º\n","        self.frequencies_of_prefixes = Counter()  # —Å—É–º–º–∞ —á–∞—Å—Ç–æ—Ç n-–≥—Ä–∞–º–º –¥–ª—è –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤\n","        self._build_model(texts)\n","\n","    def _build_model(self, texts: List[str]):\n","        for text in texts:\n","            tokens = self.vocabulary.encode(text) + [self.vocabulary.word2idx[\"<EOS>\"]]\n","            for i in range(len(tokens)):\n","                # YOUR_CODE_HERE\n","                ...\n","\n","    def _get_probability(self, prefix: List[int], token: int) -> float:\n","        # YOUR_CODE_HERE\n","        ...\n","\n","    def generate_next_token(self, prefix: List[int]) -> int:\n","        \"\"\"\n","        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É.\n","\n","        :param prefix: –ø—Ä–µ—Ñ–∏–∫—Å\n","        :return: —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω\n","        \"\"\"\n","        # YOUR_CODE_HERE\n","        ...\n","\n","    def autocomplete(self, text: str, max_len: int = 32) -> str:\n","        \"\"\"\n","        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞.\n","\n","        :param text: —Ç–µ–∫—Å—Ç\n","        :param max_len: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞\n","        :return: –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n","        \"\"\"\n","        tokens = self.vocabulary.encode(text)\n","        assert tokens\n","\n","        # YOUR_CODE_HERE\n","        ...\n","\n","        return self.vocabulary.decode(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JWhM0uOuKeiJ"},"outputs":[],"source":["texts = [\"Hello, world!\", \"I love Python!\", \"Hello, Python\"]\n","vocab = Vocabulary([\"Hello, world!\", \"I love Python!\"], min_count=1, lower=True)\n","print(vocab)\n","ngram_lm = NGramLanguageModel(2, vocab, texts)\n","assert ngram_lm.autocomplete(\"Hello, Python\", max_len=10) == \"hello , python ! <EOS>\""]},{"cell_type":"markdown","metadata":{"id":"0bvkTEhuKeiJ"},"source":["ü§î –ú–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å $n$-–≥—Ä–∞–º–º–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å, –∫–æ–≥–¥–∞ –¥–ª–∏–Ω–∞ –ø—Ä–µ—Ñ–∏–∫—Å–∞ –º–µ–Ω—å—à–µ, —á–µ–º $n-1$? –ï—Å–ª–∏ –¥–∞, —Ç–æ –∫–∞–∫? –ï—Å–ª–∏ –Ω–µ—Ç, —Ç–æ –ø–æ—á–µ–º—É?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5xqj6B2_KeiJ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"BU6q0FX1KeiK"},"source":["ü§î –ß—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥–µ—Ç, –µ—Å–ª–∏ –≤ $n$-–≥—Ä–∞–º–º–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤–∑—è—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–µ $n$?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k1o_HqQQKeiK"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"AF50TTBYKeiK"},"source":["## Word2Vec (5 –±–∞–ª–ª–æ–≤)"]},{"cell_type":"markdown","metadata":{"id":"0a6gkuuoKeiK"},"source":["–ü–æ–∑–Ω–∞–∫–æ–º–∏–º—Å—è —Å –º–æ–¥–µ–ª—å—é Word2Vec. –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –µ—Å—Ç—å –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏.\n","https://rusvectores.org/ru/models/\n","\n","–ú–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞: https://github.com/piskvorky/gensim-data\n","\n","–ù–∞—É—á–∏–º—Å—è –¥–æ—Å—Ç–∞–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤ –∏–∑ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FnOsXLqRKeiK"},"outputs":[],"source":["from gensim.models import KeyedVectors\n","import gensim.downloader as api"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ExtA-n8hKeiL"},"outputs":[],"source":["info = api.info()  # show info about available models/datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8OT8VsdKeiL"},"outputs":[],"source":["info"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JaxyXjkCKeiL"},"outputs":[],"source":["# –≤—ã –º–æ–∂–µ—Ç–µ –≤—ã–±—Ä–∞—Ç—å –ª—é–±—É—é –º–æ–¥–µ–ª—å –∏–∑ gensim, –Ω–∞–ø—Ä–∏–º–µ—Ä fasstext\n","# w2v_model = api.load(\"glove-twitter-25\")\n","w2v_model = KeyedVectors.load_word2vec_format('data/gensim/glove-twitter-25.gz')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZjQb0SRNKeiL"},"outputs":[],"source":["print(w2v_model[\"potato\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HOLBM0TwKeiM"},"outputs":[],"source":["print(w2v_model.most_similar(positive=[\"potato\", \"burger\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCBZNTraKeiM"},"outputs":[],"source":["print(w2v_model.most_similar(positive=[\"potato\", \"tomato\"]))"]},{"cell_type":"markdown","metadata":{"id":"RV88DyjBKeiM"},"source":["–†–µ–∞–ª–∏–∑—É–π—Ç–µ —Å–∞–º–æ—Å—Ç–æ—è–∏—Ç–µ–ª—å–Ω–æ –º–µ—Ç–æ–¥—ã $3CosAdd$ –∏ $3CosMul$:\n","\n","$b^* = \\arg \\max_{w \\in W} \\cos (w, a^* - a + b)$\n","\n","$b^* = \\arg \\max_{w \\in W} \\frac{\\cos(w, b) \\times \\cos(w, a^*)}{\\cos(w, a)}$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_n-TXSfKeiM"},"outputs":[],"source":["def three_cos_add(a: str, b: str, a_star: str) -> str:\n","    # YOUR_CODE_HERE\n","    ...\n","    return b_star"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EG3JchtOKeiM"},"outputs":[],"source":["def three_cos_mul(a: str, b: str, a_star: str) -> str:\n","    # YOUR_CODE_HERE\n","    ...\n","    return b_star"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YKhMW0qOKeiN"},"outputs":[],"source":["three_cos_add(\"man\", \"woman\", \"king\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aNR5EJ1NKeiN"},"outputs":[],"source":["three_cos_mul(\"man\", \"woman\", \"king\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v21ldnaqKeiT"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"98steJqJKeiT"},"source":["## –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ RNN –≤ PyTorch (5 –±–∞–ª–ª–æ–≤)"]},{"cell_type":"markdown","metadata":{"id":"T8nUpqd5KeiT"},"source":["–î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö –Ω–∞ torch –Ω—É–∂–µ–Ω —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Å–ª–æ–π Embedding.\n","–ü–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6g7UrIOKeiU"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJ0Fen1UKeiU"},"outputs":[],"source":["emd_dim = 6\n","embedding_layer = \"YOUR_CODE_HERE\"\n","\n","batch_size = 2\n","seq_len = 5\n","\n","input_ids = torch.tensor([[1, 2, 3, 4, 5], [1, 5, 6, 0, 0]]).long()\n","key_padding_mask = input_ids > 0\n","\n","assert input_ids.shape == (batch_size, seq_len)\n","\n","embeddings = \"YOUR_CODE_HERE\"\n","print(embeddings)\n","print(embeddings.shape)\n","assert embeddings.shape == (batch_size, seq_len, emd_dim)"]},{"cell_type":"markdown","metadata":{"id":"BWy4pAvnKeiU"},"source":["–í –∫–∞—á–µ—Å—Ç–≤–µ RNN –≤–æ–∑—å–º–µ–º –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é LSTM."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2yVqvv-KKeiU"},"outputs":[],"source":["hidden_size = 7\n","n_layers = 3\n","\n","rnn = YOUR_CODE_HERE\n","\n","# –ü—Ä–æ—á–∏—Ç–∞–π—Ç–µ —Ç—É—Ç–æ—Ä–∏–∞–ª –ø–æ –ø–∞–¥–¥–∏–Ω–≥—É –≤ RNN\n","# https://www.geeksforgeeks.org/how-do-you-handle-sequence-padding-and-packing-in-pytorch-for-rnns/\n","\n","packed_input = YOUR_CODE_HERE\n","rnn_outputs, (h, c) = rnn(packed_input)\n","rnn_outputs, input_sizes = YOUR_CODE_HERE\n","assert rnn_outputs.shape == (batch_size, seq_len, hidden_size * 2)"]},{"cell_type":"markdown","metadata":{"id":"uWnQEWJTKeiU"},"source":["ü§î –ü–æ—á–µ–º—É –¥–ª—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö —Å–µ—Ç–µ–π –Ω—É–∂–µ–Ω —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞–¥–¥–∏–Ω–≥?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJAIUh-jKeiV"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"qr6zmXWBKeiV"},"source":["## –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (NER) (20 –±–∞–ª–ª–æ–≤)"]},{"cell_type":"markdown","metadata":{"id":"4erqvEB-KeiV"},"source":["–¢–µ–ø–µ—Ä—å –º—ã –≥–æ—Ç–æ–≤—ã –∫ —Ç–æ–º—É, —á—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å –ø–æ–ª–Ω—É—é —Å–µ—Ç—å –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ –ø–æ—Ç–æ–∫–µ–Ω–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kgvNjgTtKeiV"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mb3jFBk2KeiV"},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"markdown","metadata":{"id":"vhnNDP_WKeiV"},"source":["### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è (10 –±–∞–ª–ª–æ–≤)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lX9_oXDhKeiW"},"outputs":[],"source":["class TokenClassificationModel(nn.Module):\n","    def __init__(self, vocab_size: int, embedding_dim: int, hidden_size: int, n_layers: int, n_classes: int):\n","        super().__init__()\n","        self.embedding = YOUR_CODE_HERE\n","        self.rnn = YOUR_CODE_HERE\n","        self.n_classes = n_classes\n","        self.fc = YOUR_CODE_HERE\n","        self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n","\n","    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n","        # YOUR_CODE_HERE\n","        ...\n","        return logits\n","\n","    def training_step(self, batch):\n","        input_ids = batch[\"input_ids\"]\n","        mask = batch[\"mask\"]\n","        labels = batch[\"labels\"]\n","\n","        # YOUR_CODE_HERE\n","        ...\n","\n","        return loss\n","\n","    def validation_step(self, batch):\n","        # YOUR_CODE_HERE\n","        ...\n","        return loss\n","\n","    def configure_optimizers(self):\n","        # YOUR_CODE_HERE\n","        ...\n","        return optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TYBV23nKeiW"},"outputs":[],"source":["def collate_fn(data):\n","    input_ids = []\n","    masks = []\n","    labels = []\n","    max_len = 0\n","    for sample in data:\n","        input_ids.append([vocab.encode_word(token) for token in sample[\"tokens\"]])\n","        labels.append([label2idx[y] for y in sample[\"labels\"]])\n","        l = len(input_ids[-1])\n","        masks.append([1] * l)\n","        max_len = max(max_len, l)\n","\n","    # YOUR_CODE_HERE\n","    ...\n","    return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8rd8ZieSKeiW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aQ_y4uJAKeiX"},"outputs":[],"source":["idx2label = [\"O\", \"B-MISC\", \"I-MISC\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]\n","label2idx = {label: i for i, label in enumerate(idx2label)}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U0Gx37CNKeiX"},"outputs":[],"source":["def read_data(path: str) -> List[Dict[str, List[str]]]:\n","    samples = []\n","    with open(path) as f:\n","        sentences = f.read().split(\"\\n\\n\")\n","    for sentence in sentences:\n","        if \"-DOCSTART-\" in sentence:\n","            continue\n","        tokens = []\n","        labels = []\n","        for line in sentence.strip().split(\"\\n\"):\n","            if not line:\n","                continue\n","            line = line.split()\n","            tokens.append(line[0])\n","            labels.append(line[-1])\n","        if not tokens:\n","            continue\n","        samples.append({\"tokens\": tokens, \"labels\": labels})\n","    return samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b85qxzFxKeiX"},"outputs":[],"source":["train_data = read_data(\"data/conll2003/train.txt\")\n","val_data = read_data(\"data/conll2003/valid.txt\")\n","test_data = read_data(\"data/conll2003/test.txt\")\n","print(len(train_data), len(val_data), len(test_data))"]},{"cell_type":"markdown","metadata":{"id":"dozNmbYFKeiY"},"source":["–ü—Ä–æ—á–∏—Ç–∞–π—Ç–µ –ø—Ä–æ —Å—Ö–µ–º—ã —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ NER: https://en.wikipedia.org/wiki/Inside‚Äìoutside‚Äìbeginning_(tagging)"]},{"cell_type":"markdown","metadata":{"id":"OdobQLpQKeiY"},"source":["ü§î –ö–∞–∫–∞—è —Å—Ö–µ–º–∞ —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —ç—Ç–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"adtcLSahKeiY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g47TQ9J8KeiZ"},"outputs":[],"source":["N_EPOCHS = YOUR_CODE_HERE\n","BATCH_SIZE = YOUR_CODE_HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BR5xm3rFKeiZ"},"outputs":[],"source":["train_dl = DataLoader(train_data, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)\n","val_dl = DataLoader(val_data, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False)\n","test_dl = DataLoader(test_data, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TyJiJHvzKeia"},"outputs":[],"source":["vocab = Vocabulary([\" \".join(example[\"tokens\"]) for example in train_data], min_count=2, lower=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RHLIGpF9Keia"},"outputs":[],"source":["len(vocab)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F0by4Gv_Keib"},"outputs":[],"source":["ner_model = YOUR_CODE_HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"gkVIOO_5Keib"},"outputs":[],"source":["# –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ word2vec\n","# YOUR_CODE_HERE\n","# –∑–∞–º–æ—Ä–æ–∑—å—Ç–µ —Å–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n","# YOUR_CODE_HERE"]},{"cell_type":"markdown","metadata":{"id":"muNB36JEKeib"},"source":["ü§î –ö–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤, –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤ –º–æ–¥–µ–ª–∏ word2vec?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"anmghsUTKeib"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"sIQHzevLKeic"},"source":["### –û–±—É—á–µ–Ω–∏–µ –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å (10 –±–∞–ª–ª–æ–≤)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBuWUpDFKeic"},"outputs":[],"source":["ner_model.to(device)\n","print(ner_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KVxxCH2tKeic"},"outputs":[],"source":["optimizer = ner_model.configure_optimizers()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XI8w4LQNKeid"},"outputs":[],"source":["for epoch in range(N_EPOCHS):\n","    ner_model.train()\n","    train_losses = []\n","    val_losses = []\n","    for i, batch in tqdm(enumerate(train_dl)):\n","        loss = ner_model.training_step(batch)\n","        train_losses.append(loss.detach().cpu().numpy())\n","        # YOUR_CODE_HERE\n","        ...\n","    ner_model.eval()\n","    with torch.no_grad():\n","        for i, batch in tqdm(enumerate(val_dl)):\n","            loss = ner_model.validation_step(batch)\n","            val_losses.append(loss.cpu().numpy())\n","    train_loss = sum(train_losses) / len(train_losses)\n","    val_loss = sum(val_losses) / len(val_losses)\n","\n","    print(f\"{epoch = }: {train_loss = }, {val_loss = }\")\n","    savepath = f\"ner_model_{epoch}ep.bin\"\n","    torch.save(ner_model.state_dict(), savepath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ffzm23uLKeid"},"outputs":[],"source":["def predict(model, test_data) -> List[List[str]]:\n","    with torch.no_grad():\n","        model.eval()\n","        predictions = []\n","\n","        # YOUR_CODE_HERE\n","        ...\n","        return predictions  # –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ —Ç–µ–≥–∏"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xprp6FCnKeid"},"outputs":[],"source":["predictions = predict(ner_model, test_dl)\n","for sample, pred in zip(test_data, predictions):\n","    text = \" \".join(sample[\"tokens\"])\n","    print(\"*\" * 100)\n","    print(text)\n","    for tag, start, end in pred:\n","        print(tag, \" \".join(sample[\"tokens\"][start:end + 1]))"]},{"cell_type":"markdown","metadata":{"id":"ja28tqMLKeid"},"source":["–ò—Å–ø–æ–ª—å–∑—É—è –±–∏–±–ª–∏–æ—Ç–µ–∫—É https://github.com/chakki-works/seqeval, —Ä–∞—Å—Å—á–∏—Ç–∞–π—Ç–µ –º–µ—Ç—Ä–∏–∫–∏.\n","\n","–†–µ–∑—É–ª—å—Ç–∞—Ç—ã SotA-—Ä–µ—à–µ–Ω–∏–π –≤—ã –Ω–∞–π–¥–µ—Ç–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ https://paperswithcode.com/sota/named-entity-recognition-ner-on-conll-2003."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJbhbmBqKeie"},"outputs":[],"source":["# YOUR_CODE_HERE\n","report = YOUR_CODE_HERE\n","print(report)"]},{"cell_type":"markdown","metadata":{"id":"mQHI7XeIKeie"},"source":["### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–¥–∞–Ω–∏–µ (–Ω–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è)"]},{"cell_type":"markdown","metadata":{"id":"vCaCvQ9SKeie"},"source":["(–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–≤—ã—Å–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏. –í–æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–¥–µ–π:\n","\n","1. –í–∑—è—Ç—å –±–æ–ª–µ–µ —Ç—è–∂–µ–ª—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.\n","2. –ù–∞ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞—Ç—å —Å–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.\n","3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å MLP –ø–æ—Å–ª–µ RNN.\n","4. –£–º–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤, –¥—Ä–æ–ø–∞—É—Ç, –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, —Å–∫–µ–¥—É–ª–µ—Ä –∏ —Ç. –¥."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8iAmefjCKeie"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJXIKyfnKeif"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}